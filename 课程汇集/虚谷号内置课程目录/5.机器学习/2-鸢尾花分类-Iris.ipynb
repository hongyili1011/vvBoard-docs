{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 神经网络和机器学习之鸢尾花分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "案例说明：鸢尾花(Iris)分类，使用全连接神经网络层。\n",
    "\n",
    "鸢(yuān)尾花分类相当于机器学习中的Helloworld问题。鸢尾花可以分为很多类，一般通过花萼长度、花萼宽度、花瓣长度、花瓣宽度进行区分。我们让机器来学习关于这个鸢尾花分类的一组数据，然后建立模型，训练。后面直接给出花的四个特征，让机器判断花的分类。\n",
    "\n",
    "案例选择了keras框架，需要先安装keras和tensorflow。虚谷号教育版已经预装必要的库，可以直接使用。\n",
    "\n",
    "本案例已经提供了训练好的模型，放在model文件夹中，文件名称为：2-model-vv.h5。如果想直接测试模型，请跳到“导入模型”环节，输入数据开始识别。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.环境搭建\n",
    "\n",
    "下面是安装命令：\n",
    "\n",
    "pip install keras\n",
    "\n",
    "pip install -i https://pypi.tuna.tsinghua.edu.cn/simple tensorflow\n",
    "\n",
    "建议选择清华源，速度将快很多。参考命令如下：\n",
    "\n",
    "pip install -i https://pypi.tuna.tsinghua.edu.cn/simple tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.数据准备\n",
    "\n",
    "\n",
    "鸢尾花分类数据集在`data`中，文件名称为`iris.csv`。数据分为5列，前4列为花萼长度，花萼宽度，花瓣长度，花瓣宽度等4个用于识别鸢尾花的属性，第5列为鸢尾花的类别（包括Setosa，Versicolour，Virginica三类）。\n",
    "\n",
    "这个数据集可以从UCI数据集上直接下载，具体地址为：http://archive.ics.uci.edu/ml/datasets/Iris\n",
    "打开页面后点击Datafolder就可以下载到本地磁盘上，默认格式为逗号分隔的文本文件。\n",
    "\n",
    "也可以直接从sklearn包里datasets里导入，语法为：from sklearn.datasets import load_iris。\n",
    "如果从本地磁盘上读入该数据集，可以采用pandas包里的read_excel或者read_csv方法，也可以利用python里面的csv包来处理。\n",
    "\n",
    "开始导入数据集吧。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data=pd.read_csv('./data/iris.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "该问题属于比较典型的多分类问题，因此在训练数据预处理中，首先对分类结果标签\"Species\"进行独热编码化。所谓独热编码(One-Hot)，是指用0/1构成的数组来表示一种情况，比如在鸢尾花分类中，顺序编码可以用0、1、2来表示不同的鸢尾花品种，而独热编码可以用[1,0,0]表示setosa，用[0,1,0]表示versicolor,用[0,0,1]表示virginica。独热编码相对于顺序编码避免了神经网络把没有数值大小意义的数据错误的理解为有数值意义。比如如果用顺序编码来表示鸢尾花品种，神经网络会错误的认为2表示的品种与0表示的品种之间的差距比较大，而与1表示的品种差距比较小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.get_dummies(data,columns=['Species']) #把种类(列名称为“Species”)进行独热编码\n",
    "x=data[['Sepal.Length', 'Sepal.Width', 'Petal.Length','Petal.Width']]\n",
    "y=data.iloc[:,-3:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此时x与y的形状分别是(150,3)和(150,1)，即x具有150行、3列，y具有150行、1列。其中x是输入的数据(鸢尾花的属性)，y是输出的结果(鸢尾花的类别)。\n",
    "输出来看一下，你会发现x与y的行数一定是相同的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sepal.Length</th>\n",
       "      <th>Sepal.Width</th>\n",
       "      <th>Petal.Length</th>\n",
       "      <th>Petal.Width</th>\n",
       "      <th>Species_Iris-setosa</th>\n",
       "      <th>Species_Iris-versicolor</th>\n",
       "      <th>Species_Iris-virginica</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.5</td>\n",
       "      <td>2.6</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.1</td>\n",
       "      <td>2.6</td>\n",
       "      <td>5.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.8</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sepal.Length  Sepal.Width  Petal.Length  Petal.Width  Species_Iris-setosa  \\\n",
       "0           4.6          3.1           1.5          0.2                    1   \n",
       "1           5.5          2.6           4.4          1.2                    0   \n",
       "2           6.1          2.6           5.6          1.4                    0   \n",
       "3           7.2          3.0           5.8          1.6                    0   \n",
       "4           5.2          3.4           1.4          0.2                    1   \n",
       "\n",
       "   Species_Iris-versicolor  Species_Iris-virginica  \n",
       "0                        0                       0  \n",
       "1                        1                       0  \n",
       "2                        0                       1  \n",
       "3                        0                       1  \n",
       "4                        0                       0  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sepal.Length</th>\n",
       "      <th>Sepal.Width</th>\n",
       "      <th>Petal.Length</th>\n",
       "      <th>Petal.Width</th>\n",
       "      <th>Species_Iris-setosa</th>\n",
       "      <th>Species_Iris-versicolor</th>\n",
       "      <th>Species_Iris-virginica</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.1</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Sepal.Length  Sepal.Width  Petal.Length  Petal.Width  \\\n",
       "145           6.9          3.1           5.4          2.1   \n",
       "146           6.0          3.0           4.8          1.8   \n",
       "147           6.9          3.1           5.1          2.3   \n",
       "148           5.1          3.7           1.5          0.4   \n",
       "149           5.4          3.9           1.7          0.4   \n",
       "\n",
       "     Species_Iris-setosa  Species_Iris-versicolor  Species_Iris-virginica  \n",
       "145                    0                        0                       1  \n",
       "146                    0                        0                       1  \n",
       "147                    0                        0                       1  \n",
       "148                    1                        0                       0  \n",
       "149                    1                        0                       0  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 150)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x),len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.建立模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "多分类问题是二分类问题的扩展。当分类数大于2时，就是多分类问题。比如把笔分成铅笔、圆珠笔、钢笔等等，就是多分类问题。多分类问题需要神经网络将最后一层神经元个数设置为与分类数目相同以输出一个数组，这个数组的长度就是分类数目，数组中每个数值对应在不同类别上的可能性。一般的，多分类问题通过softmax函数激活，损失函数使用类别交叉熵损失(categorical_crossentropy)。\n",
    "\n",
    "keras支持很多类型的神经网络层，这里使用add方法添加2个全连接神经网络层（Dense层）。\n",
    "第一层通过input_dim参数指定接收输入数据的维度为4（鸢尾花的属性），units=8表示将这个4维数据全连接到8个神经元，activation定义了激活函数为relu。第二层神经元，也就是最后一层神经元的个数设置要和分类的数目相同，所以设置为3，激活函数为softmax。\n",
    "\n",
    "代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=keras.models.Sequential()\n",
    "model.add(layers.Dense(units=8, input_dim=4, activation='relu'))\n",
    "model.add(layers.Dense(units=3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义好模型的层之后，需要对模型进行编译，同时指定训练模型所需要的优化器以及损失的估算方法。在keras中，可以通过optimizer参数来指定优化器。这里选择了adam。loss定义了损失函数为category_crossentropy。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后对模型进行训练，一下代码利用现有数据x和y对模型进行训练500次，epochs表示训练轮次，batch_size表示每次有多少行数据参与训练，最后把整个训练过程记录到history中。程序运行后，在控制台会打印出每轮次的训练情况。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "150/150 [==============================] - 1s 8ms/step - loss: 2.4921\n",
      "Epoch 2/500\n",
      "150/150 [==============================] - 0s 82us/step - loss: 2.4746\n",
      "Epoch 3/500\n",
      "150/150 [==============================] - 0s 72us/step - loss: 2.4545\n",
      "Epoch 4/500\n",
      "150/150 [==============================] - 0s 57us/step - loss: 2.4312\n",
      "Epoch 5/500\n",
      "150/150 [==============================] - 0s 126us/step - loss: 2.4080\n",
      "Epoch 6/500\n",
      "150/150 [==============================] - 0s 57us/step - loss: 2.3849\n",
      "Epoch 7/500\n",
      "150/150 [==============================] - 0s 56us/step - loss: 2.3620\n",
      "Epoch 8/500\n",
      "150/150 [==============================] - 0s 56us/step - loss: 2.3392\n",
      "Epoch 9/500\n",
      "150/150 [==============================] - 0s 59us/step - loss: 2.3165\n",
      "Epoch 10/500\n",
      "150/150 [==============================] - 0s 56us/step - loss: 2.2942\n",
      "Epoch 11/500\n",
      "150/150 [==============================] - 0s 61us/step - loss: 2.2718\n",
      "Epoch 12/500\n",
      "150/150 [==============================] - 0s 86us/step - loss: 2.2496\n",
      "Epoch 13/500\n",
      "150/150 [==============================] - 0s 66us/step - loss: 2.2274\n",
      "Epoch 14/500\n",
      "150/150 [==============================] - 0s 63us/step - loss: 2.2055\n",
      "Epoch 15/500\n",
      "150/150 [==============================] - 0s 67us/step - loss: 2.1837\n",
      "Epoch 16/500\n",
      "150/150 [==============================] - 0s 87us/step - loss: 2.1620\n",
      "Epoch 17/500\n",
      "150/150 [==============================] - 0s 74us/step - loss: 2.1405\n",
      "Epoch 18/500\n",
      "150/150 [==============================] - 0s 63us/step - loss: 2.1191\n",
      "Epoch 19/500\n",
      "150/150 [==============================] - 0s 59us/step - loss: 2.0978\n",
      "Epoch 20/500\n",
      "150/150 [==============================] - 0s 83us/step - loss: 2.0767\n",
      "Epoch 21/500\n",
      "150/150 [==============================] - 0s 68us/step - loss: 2.0558\n",
      "Epoch 22/500\n",
      "150/150 [==============================] - 0s 78us/step - loss: 2.0350\n",
      "Epoch 23/500\n",
      "150/150 [==============================] - 0s 63us/step - loss: 2.0144\n",
      "Epoch 24/500\n",
      "150/150 [==============================] - 0s 63us/step - loss: 1.9940\n",
      "Epoch 25/500\n",
      "150/150 [==============================] - 0s 80us/step - loss: 1.9738\n",
      "Epoch 26/500\n",
      "150/150 [==============================] - 0s 57us/step - loss: 1.9537\n",
      "Epoch 27/500\n",
      "150/150 [==============================] - 0s 61us/step - loss: 1.9339\n",
      "Epoch 28/500\n",
      "150/150 [==============================] - 0s 132us/step - loss: 1.9142\n",
      "Epoch 29/500\n",
      "150/150 [==============================] - 0s 59us/step - loss: 1.8945\n",
      "Epoch 30/500\n",
      "150/150 [==============================] - 0s 62us/step - loss: 1.8752\n",
      "Epoch 31/500\n",
      "150/150 [==============================] - 0s 71us/step - loss: 1.8561\n",
      "Epoch 32/500\n",
      "150/150 [==============================] - 0s 79us/step - loss: 1.8372\n",
      "Epoch 33/500\n",
      "150/150 [==============================] - 0s 79us/step - loss: 1.8184\n",
      "Epoch 34/500\n",
      "150/150 [==============================] - 0s 139us/step - loss: 1.7999\n",
      "Epoch 35/500\n",
      "150/150 [==============================] - 0s 77us/step - loss: 1.7813\n",
      "Epoch 36/500\n",
      "150/150 [==============================] - 0s 110us/step - loss: 1.7632\n",
      "Epoch 37/500\n",
      "150/150 [==============================] - 0s 71us/step - loss: 1.7452\n",
      "Epoch 38/500\n",
      "150/150 [==============================] - 0s 84us/step - loss: 1.7275\n",
      "Epoch 39/500\n",
      "150/150 [==============================] - 0s 64us/step - loss: 1.7099\n",
      "Epoch 40/500\n",
      "150/150 [==============================] - 0s 60us/step - loss: 1.6925\n",
      "Epoch 41/500\n",
      "150/150 [==============================] - 0s 77us/step - loss: 1.6753\n",
      "Epoch 42/500\n",
      "150/150 [==============================] - 0s 70us/step - loss: 1.6583\n",
      "Epoch 43/500\n",
      "150/150 [==============================] - 0s 58us/step - loss: 1.6415\n",
      "Epoch 44/500\n",
      "150/150 [==============================] - 0s 60us/step - loss: 1.6249\n",
      "Epoch 45/500\n",
      "150/150 [==============================] - 0s 75us/step - loss: 1.6086\n",
      "Epoch 46/500\n",
      "150/150 [==============================] - 0s 80us/step - loss: 1.5924\n",
      "Epoch 47/500\n",
      "150/150 [==============================] - 0s 76us/step - loss: 1.5765\n",
      "Epoch 48/500\n",
      "150/150 [==============================] - 0s 85us/step - loss: 1.5607\n",
      "Epoch 49/500\n",
      "150/150 [==============================] - 0s 64us/step - loss: 1.5451\n",
      "Epoch 50/500\n",
      "150/150 [==============================] - 0s 61us/step - loss: 1.5298\n",
      "Epoch 51/500\n",
      "150/150 [==============================] - 0s 76us/step - loss: 1.5145\n",
      "Epoch 52/500\n",
      "150/150 [==============================] - 0s 85us/step - loss: 1.4995\n",
      "Epoch 53/500\n",
      "150/150 [==============================] - 0s 64us/step - loss: 1.4845\n",
      "Epoch 54/500\n",
      "150/150 [==============================] - 0s 63us/step - loss: 1.4699\n",
      "Epoch 55/500\n",
      "150/150 [==============================] - 0s 70us/step - loss: 1.4555\n",
      "Epoch 56/500\n",
      "150/150 [==============================] - 0s 62us/step - loss: 1.4412\n",
      "Epoch 57/500\n",
      "150/150 [==============================] - 0s 73us/step - loss: 1.4269\n",
      "Epoch 58/500\n",
      "150/150 [==============================] - 0s 75us/step - loss: 1.4128\n",
      "Epoch 59/500\n",
      "150/150 [==============================] - 0s 71us/step - loss: 1.3989\n",
      "Epoch 60/500\n",
      "150/150 [==============================] - 0s 60us/step - loss: 1.3851\n",
      "Epoch 61/500\n",
      "150/150 [==============================] - 0s 65us/step - loss: 1.3713\n",
      "Epoch 62/500\n",
      "150/150 [==============================] - 0s 60us/step - loss: 1.3574\n",
      "Epoch 63/500\n",
      "150/150 [==============================] - 0s 67us/step - loss: 1.3436\n",
      "Epoch 64/500\n",
      "150/150 [==============================] - 0s 88us/step - loss: 1.3299\n",
      "Epoch 65/500\n",
      "150/150 [==============================] - 0s 76us/step - loss: 1.3163\n",
      "Epoch 66/500\n",
      "150/150 [==============================] - 0s 71us/step - loss: 1.3026\n",
      "Epoch 67/500\n",
      "150/150 [==============================] - 0s 68us/step - loss: 1.2889\n",
      "Epoch 68/500\n",
      "150/150 [==============================] - 0s 68us/step - loss: 1.2755\n",
      "Epoch 69/500\n",
      "150/150 [==============================] - 0s 64us/step - loss: 1.2621\n",
      "Epoch 70/500\n",
      "150/150 [==============================] - 0s 88us/step - loss: 1.2489\n",
      "Epoch 71/500\n",
      "150/150 [==============================] - 0s 75us/step - loss: 1.2363\n",
      "Epoch 72/500\n",
      "150/150 [==============================] - 0s 60us/step - loss: 1.2240\n",
      "Epoch 73/500\n",
      "150/150 [==============================] - 0s 65us/step - loss: 1.2122\n",
      "Epoch 74/500\n",
      "150/150 [==============================] - 0s 89us/step - loss: 1.2008\n",
      "Epoch 75/500\n",
      "150/150 [==============================] - 0s 68us/step - loss: 1.1899\n",
      "Epoch 76/500\n",
      "150/150 [==============================] - 0s 62us/step - loss: 1.1795\n",
      "Epoch 77/500\n",
      "150/150 [==============================] - 0s 66us/step - loss: 1.1696\n",
      "Epoch 78/500\n",
      "150/150 [==============================] - 0s 76us/step - loss: 1.1600\n",
      "Epoch 79/500\n",
      "150/150 [==============================] - 0s 73us/step - loss: 1.1508\n",
      "Epoch 80/500\n",
      "150/150 [==============================] - 0s 85us/step - loss: 1.1422\n",
      "Epoch 81/500\n",
      "150/150 [==============================] - 0s 58us/step - loss: 1.1338\n",
      "Epoch 82/500\n",
      "150/150 [==============================] - 0s 64us/step - loss: 1.1257\n",
      "Epoch 83/500\n",
      "150/150 [==============================] - 0s 74us/step - loss: 1.1178\n",
      "Epoch 84/500\n",
      "150/150 [==============================] - 0s 72us/step - loss: 1.1100\n",
      "Epoch 85/500\n",
      "150/150 [==============================] - 0s 60us/step - loss: 1.1025\n",
      "Epoch 86/500\n",
      "150/150 [==============================] - 0s 64us/step - loss: 1.0951\n",
      "Epoch 87/500\n",
      "150/150 [==============================] - 0s 81us/step - loss: 1.0879\n",
      "Epoch 88/500\n",
      "150/150 [==============================] - 0s 90us/step - loss: 1.0809\n",
      "Epoch 89/500\n",
      "150/150 [==============================] - 0s 63us/step - loss: 1.0739\n",
      "Epoch 90/500\n",
      "150/150 [==============================] - 0s 55us/step - loss: 1.0672\n",
      "Epoch 91/500\n",
      "150/150 [==============================] - 0s 61us/step - loss: 1.0606\n",
      "Epoch 92/500\n",
      "150/150 [==============================] - 0s 60us/step - loss: 1.0542\n",
      "Epoch 93/500\n",
      "150/150 [==============================] - 0s 69us/step - loss: 1.0479\n",
      "Epoch 94/500\n",
      "150/150 [==============================] - 0s 77us/step - loss: 1.0417\n",
      "Epoch 95/500\n",
      "150/150 [==============================] - 0s 72us/step - loss: 1.0357\n",
      "Epoch 96/500\n",
      "150/150 [==============================] - 0s 65us/step - loss: 1.0298\n",
      "Epoch 97/500\n",
      "150/150 [==============================] - 0s 95us/step - loss: 1.0240\n",
      "Epoch 98/500\n",
      "150/150 [==============================] - 0s 53us/step - loss: 1.0184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99/500\n",
      "150/150 [==============================] - 0s 83us/step - loss: 1.0129\n",
      "Epoch 100/500\n",
      "150/150 [==============================] - 0s 64us/step - loss: 1.0075\n",
      "Epoch 101/500\n",
      "150/150 [==============================] - 0s 72us/step - loss: 1.0023\n",
      "Epoch 102/500\n",
      "150/150 [==============================] - 0s 90us/step - loss: 0.9972\n",
      "Epoch 103/500\n",
      "150/150 [==============================] - 0s 72us/step - loss: 0.9923\n",
      "Epoch 104/500\n",
      "150/150 [==============================] - 0s 67us/step - loss: 0.9874\n",
      "Epoch 105/500\n",
      "150/150 [==============================] - 0s 62us/step - loss: 0.9827\n",
      "Epoch 106/500\n",
      "150/150 [==============================] - 0s 70us/step - loss: 0.9781\n",
      "Epoch 107/500\n",
      "150/150 [==============================] - 0s 79us/step - loss: 0.9736\n",
      "Epoch 108/500\n",
      "150/150 [==============================] - 0s 68us/step - loss: 0.9693\n",
      "Epoch 109/500\n",
      "150/150 [==============================] - 0s 75us/step - loss: 0.9650\n",
      "Epoch 110/500\n",
      "150/150 [==============================] - 0s 65us/step - loss: 0.9608\n",
      "Epoch 111/500\n",
      "150/150 [==============================] - 0s 83us/step - loss: 0.9568\n",
      "Epoch 112/500\n",
      "150/150 [==============================] - 0s 85us/step - loss: 0.9528\n",
      "Epoch 113/500\n",
      "150/150 [==============================] - 0s 57us/step - loss: 0.9489\n",
      "Epoch 114/500\n",
      "150/150 [==============================] - 0s 60us/step - loss: 0.9452\n",
      "Epoch 115/500\n",
      "150/150 [==============================] - 0s 49us/step - loss: 0.9415\n",
      "Epoch 116/500\n",
      "150/150 [==============================] - 0s 71us/step - loss: 0.9379\n",
      "Epoch 117/500\n",
      "150/150 [==============================] - 0s 79us/step - loss: 0.9344\n",
      "Epoch 118/500\n",
      "150/150 [==============================] - 0s 81us/step - loss: 0.9310\n",
      "Epoch 119/500\n",
      "150/150 [==============================] - 0s 67us/step - loss: 0.9276\n",
      "Epoch 120/500\n",
      "150/150 [==============================] - 0s 85us/step - loss: 0.9244\n",
      "Epoch 121/500\n",
      "150/150 [==============================] - 0s 90us/step - loss: 0.9212\n",
      "Epoch 122/500\n",
      "150/150 [==============================] - 0s 84us/step - loss: 0.9182\n",
      "Epoch 123/500\n",
      "150/150 [==============================] - 0s 86us/step - loss: 0.9151\n",
      "Epoch 124/500\n",
      "150/150 [==============================] - 0s 72us/step - loss: 0.9122\n",
      "Epoch 125/500\n",
      "150/150 [==============================] - 0s 68us/step - loss: 0.9093\n",
      "Epoch 126/500\n",
      "150/150 [==============================] - 0s 101us/step - loss: 0.9065\n",
      "Epoch 127/500\n",
      "150/150 [==============================] - 0s 83us/step - loss: 0.9038\n",
      "Epoch 128/500\n",
      "150/150 [==============================] - 0s 87us/step - loss: 0.9011\n",
      "Epoch 129/500\n",
      "150/150 [==============================] - 0s 81us/step - loss: 0.8985\n",
      "Epoch 130/500\n",
      "150/150 [==============================] - 0s 80us/step - loss: 0.8959\n",
      "Epoch 131/500\n",
      "150/150 [==============================] - 0s 76us/step - loss: 0.8934\n",
      "Epoch 132/500\n",
      "150/150 [==============================] - 0s 69us/step - loss: 0.8909\n",
      "Epoch 133/500\n",
      "150/150 [==============================] - 0s 64us/step - loss: 0.8885\n",
      "Epoch 134/500\n",
      "150/150 [==============================] - 0s 59us/step - loss: 0.8861\n",
      "Epoch 135/500\n",
      "150/150 [==============================] - 0s 69us/step - loss: 0.8837\n",
      "Epoch 136/500\n",
      "150/150 [==============================] - 0s 85us/step - loss: 0.8814\n",
      "Epoch 137/500\n",
      "150/150 [==============================] - 0s 72us/step - loss: 0.8792\n",
      "Epoch 138/500\n",
      "150/150 [==============================] - 0s 70us/step - loss: 0.8769\n",
      "Epoch 139/500\n",
      "150/150 [==============================] - 0s 76us/step - loss: 0.8747\n",
      "Epoch 140/500\n",
      "150/150 [==============================] - 0s 78us/step - loss: 0.8726\n",
      "Epoch 141/500\n",
      "150/150 [==============================] - 0s 81us/step - loss: 0.8705\n",
      "Epoch 142/500\n",
      "150/150 [==============================] - 0s 80us/step - loss: 0.8684\n",
      "Epoch 143/500\n",
      "150/150 [==============================] - 0s 60us/step - loss: 0.8663\n",
      "Epoch 144/500\n",
      "150/150 [==============================] - 0s 66us/step - loss: 0.8643\n",
      "Epoch 145/500\n",
      "150/150 [==============================] - 0s 81us/step - loss: 0.8622\n",
      "Epoch 146/500\n",
      "150/150 [==============================] - 0s 78us/step - loss: 0.8602\n",
      "Epoch 147/500\n",
      "150/150 [==============================] - 0s 62us/step - loss: 0.8583\n",
      "Epoch 148/500\n",
      "150/150 [==============================] - 0s 77us/step - loss: 0.8563\n",
      "Epoch 149/500\n",
      "150/150 [==============================] - 0s 77us/step - loss: 0.8544\n",
      "Epoch 150/500\n",
      "150/150 [==============================] - 0s 76us/step - loss: 0.8525\n",
      "Epoch 151/500\n",
      "150/150 [==============================] - 0s 75us/step - loss: 0.8506\n",
      "Epoch 152/500\n",
      "150/150 [==============================] - 0s 68us/step - loss: 0.8488\n",
      "Epoch 153/500\n",
      "150/150 [==============================] - 0s 69us/step - loss: 0.8469\n",
      "Epoch 154/500\n",
      "150/150 [==============================] - 0s 79us/step - loss: 0.8451\n",
      "Epoch 155/500\n",
      "150/150 [==============================] - 0s 68us/step - loss: 0.8433\n",
      "Epoch 156/500\n",
      "150/150 [==============================] - 0s 86us/step - loss: 0.8415\n",
      "Epoch 157/500\n",
      "150/150 [==============================] - 0s 71us/step - loss: 0.8398\n",
      "Epoch 158/500\n",
      "150/150 [==============================] - 0s 92us/step - loss: 0.8380\n",
      "Epoch 159/500\n",
      "150/150 [==============================] - 0s 53us/step - loss: 0.8363\n",
      "Epoch 160/500\n",
      "150/150 [==============================] - 0s 65us/step - loss: 0.8346\n",
      "Epoch 161/500\n",
      "150/150 [==============================] - 0s 74us/step - loss: 0.8328\n",
      "Epoch 162/500\n",
      "150/150 [==============================] - 0s 85us/step - loss: 0.8311\n",
      "Epoch 163/500\n",
      "150/150 [==============================] - 0s 75us/step - loss: 0.8295\n",
      "Epoch 164/500\n",
      "150/150 [==============================] - 0s 79us/step - loss: 0.8278\n",
      "Epoch 165/500\n",
      "150/150 [==============================] - 0s 72us/step - loss: 0.8261\n",
      "Epoch 166/500\n",
      "150/150 [==============================] - 0s 58us/step - loss: 0.8245\n",
      "Epoch 167/500\n",
      "150/150 [==============================] - 0s 80us/step - loss: 0.8229\n",
      "Epoch 168/500\n",
      "150/150 [==============================] - 0s 92us/step - loss: 0.8212\n",
      "Epoch 169/500\n",
      "150/150 [==============================] - 0s 72us/step - loss: 0.8196\n",
      "Epoch 170/500\n",
      "150/150 [==============================] - 0s 98us/step - loss: 0.8180\n",
      "Epoch 171/500\n",
      "150/150 [==============================] - 0s 66us/step - loss: 0.8164\n",
      "Epoch 172/500\n",
      "150/150 [==============================] - 0s 77us/step - loss: 0.8149\n",
      "Epoch 173/500\n",
      "150/150 [==============================] - 0s 81us/step - loss: 0.8133\n",
      "Epoch 174/500\n",
      "150/150 [==============================] - 0s 84us/step - loss: 0.8117\n",
      "Epoch 175/500\n",
      "150/150 [==============================] - 0s 55us/step - loss: 0.8102\n",
      "Epoch 176/500\n",
      "150/150 [==============================] - 0s 72us/step - loss: 0.8086\n",
      "Epoch 177/500\n",
      "150/150 [==============================] - 0s 78us/step - loss: 0.8071\n",
      "Epoch 178/500\n",
      "150/150 [==============================] - 0s 72us/step - loss: 0.8056\n",
      "Epoch 179/500\n",
      "150/150 [==============================] - 0s 82us/step - loss: 0.8041\n",
      "Epoch 180/500\n",
      "150/150 [==============================] - 0s 69us/step - loss: 0.8026\n",
      "Epoch 181/500\n",
      "150/150 [==============================] - 0s 67us/step - loss: 0.8011\n",
      "Epoch 182/500\n",
      "150/150 [==============================] - 0s 53us/step - loss: 0.7996\n",
      "Epoch 183/500\n",
      "150/150 [==============================] - 0s 76us/step - loss: 0.7981\n",
      "Epoch 184/500\n",
      "150/150 [==============================] - 0s 75us/step - loss: 0.7967\n",
      "Epoch 185/500\n",
      "150/150 [==============================] - 0s 62us/step - loss: 0.7952\n",
      "Epoch 186/500\n",
      "150/150 [==============================] - 0s 58us/step - loss: 0.7938\n",
      "Epoch 187/500\n",
      "150/150 [==============================] - 0s 72us/step - loss: 0.7923\n",
      "Epoch 188/500\n",
      "150/150 [==============================] - 0s 72us/step - loss: 0.7909\n",
      "Epoch 189/500\n",
      "150/150 [==============================] - 0s 62us/step - loss: 0.7895\n",
      "Epoch 190/500\n",
      "150/150 [==============================] - 0s 71us/step - loss: 0.7880\n",
      "Epoch 191/500\n",
      "150/150 [==============================] - 0s 83us/step - loss: 0.7866\n",
      "Epoch 192/500\n",
      "150/150 [==============================] - 0s 66us/step - loss: 0.7852\n",
      "Epoch 193/500\n",
      "150/150 [==============================] - 0s 76us/step - loss: 0.7838\n",
      "Epoch 194/500\n",
      "150/150 [==============================] - 0s 56us/step - loss: 0.7824\n",
      "Epoch 195/500\n",
      "150/150 [==============================] - 0s 52us/step - loss: 0.7811\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 196/500\n",
      "150/150 [==============================] - 0s 79us/step - loss: 0.7797\n",
      "Epoch 197/500\n",
      "150/150 [==============================] - 0s 74us/step - loss: 0.7783\n",
      "Epoch 198/500\n",
      "150/150 [==============================] - 0s 76us/step - loss: 0.7770\n",
      "Epoch 199/500\n",
      "150/150 [==============================] - 0s 68us/step - loss: 0.7756\n",
      "Epoch 200/500\n",
      "150/150 [==============================] - 0s 83us/step - loss: 0.7743\n",
      "Epoch 201/500\n",
      "150/150 [==============================] - 0s 72us/step - loss: 0.7729\n",
      "Epoch 202/500\n",
      "150/150 [==============================] - 0s 72us/step - loss: 0.7716\n",
      "Epoch 203/500\n",
      "150/150 [==============================] - 0s 70us/step - loss: 0.7703\n",
      "Epoch 204/500\n",
      "150/150 [==============================] - 0s 86us/step - loss: 0.7690\n",
      "Epoch 205/500\n",
      "150/150 [==============================] - 0s 85us/step - loss: 0.7677\n",
      "Epoch 206/500\n",
      "150/150 [==============================] - 0s 78us/step - loss: 0.7664\n",
      "Epoch 207/500\n",
      "150/150 [==============================] - 0s 83us/step - loss: 0.7651\n",
      "Epoch 208/500\n",
      "150/150 [==============================] - 0s 82us/step - loss: 0.7638\n",
      "Epoch 209/500\n",
      "150/150 [==============================] - 0s 84us/step - loss: 0.7625\n",
      "Epoch 210/500\n",
      "150/150 [==============================] - 0s 89us/step - loss: 0.7612\n",
      "Epoch 211/500\n",
      "150/150 [==============================] - 0s 70us/step - loss: 0.7599\n",
      "Epoch 212/500\n",
      "150/150 [==============================] - 0s 75us/step - loss: 0.7587\n",
      "Epoch 213/500\n",
      "150/150 [==============================] - 0s 78us/step - loss: 0.7574\n",
      "Epoch 214/500\n",
      "150/150 [==============================] - 0s 101us/step - loss: 0.7561\n",
      "Epoch 215/500\n",
      "150/150 [==============================] - 0s 72us/step - loss: 0.7549\n",
      "Epoch 216/500\n",
      "150/150 [==============================] - 0s 82us/step - loss: 0.7536\n",
      "Epoch 217/500\n",
      "150/150 [==============================] - 0s 61us/step - loss: 0.7524\n",
      "Epoch 218/500\n",
      "150/150 [==============================] - 0s 69us/step - loss: 0.7512\n",
      "Epoch 219/500\n",
      "150/150 [==============================] - 0s 75us/step - loss: 0.7500\n",
      "Epoch 220/500\n",
      "150/150 [==============================] - 0s 72us/step - loss: 0.7487\n",
      "Epoch 221/500\n",
      "150/150 [==============================] - 0s 77us/step - loss: 0.7475\n",
      "Epoch 222/500\n",
      "150/150 [==============================] - 0s 65us/step - loss: 0.7463\n",
      "Epoch 223/500\n",
      "150/150 [==============================] - 0s 55us/step - loss: 0.7451\n",
      "Epoch 224/500\n",
      "150/150 [==============================] - 0s 65us/step - loss: 0.7439\n",
      "Epoch 225/500\n",
      "150/150 [==============================] - 0s 59us/step - loss: 0.7427\n",
      "Epoch 226/500\n",
      "150/150 [==============================] - 0s 75us/step - loss: 0.7415\n",
      "Epoch 227/500\n",
      "150/150 [==============================] - 0s 93us/step - loss: 0.7404\n",
      "Epoch 228/500\n",
      "150/150 [==============================] - 0s 78us/step - loss: 0.7392\n",
      "Epoch 229/500\n",
      "150/150 [==============================] - 0s 72us/step - loss: 0.7380\n",
      "Epoch 230/500\n",
      "150/150 [==============================] - 0s 64us/step - loss: 0.7368\n",
      "Epoch 231/500\n",
      "150/150 [==============================] - 0s 83us/step - loss: 0.7357\n",
      "Epoch 232/500\n",
      "150/150 [==============================] - 0s 68us/step - loss: 0.7345\n",
      "Epoch 233/500\n",
      "150/150 [==============================] - 0s 62us/step - loss: 0.7334\n",
      "Epoch 234/500\n",
      "150/150 [==============================] - 0s 59us/step - loss: 0.7322\n",
      "Epoch 235/500\n",
      "150/150 [==============================] - 0s 60us/step - loss: 0.7311\n",
      "Epoch 236/500\n",
      "150/150 [==============================] - 0s 64us/step - loss: 0.7300\n",
      "Epoch 237/500\n",
      "150/150 [==============================] - 0s 76us/step - loss: 0.7288\n",
      "Epoch 238/500\n",
      "150/150 [==============================] - 0s 84us/step - loss: 0.7277\n",
      "Epoch 239/500\n",
      "150/150 [==============================] - 0s 70us/step - loss: 0.7266\n",
      "Epoch 240/500\n",
      "150/150 [==============================] - 0s 90us/step - loss: 0.7255\n",
      "Epoch 241/500\n",
      "150/150 [==============================] - 0s 76us/step - loss: 0.7244\n",
      "Epoch 242/500\n",
      "150/150 [==============================] - 0s 67us/step - loss: 0.7233\n",
      "Epoch 243/500\n",
      "150/150 [==============================] - 0s 83us/step - loss: 0.7222\n",
      "Epoch 244/500\n",
      "150/150 [==============================] - 0s 79us/step - loss: 0.7211\n",
      "Epoch 245/500\n",
      "150/150 [==============================] - 0s 68us/step - loss: 0.7200\n",
      "Epoch 246/500\n",
      "150/150 [==============================] - 0s 63us/step - loss: 0.7189\n",
      "Epoch 247/500\n",
      "150/150 [==============================] - 0s 64us/step - loss: 0.7178\n",
      "Epoch 248/500\n",
      "150/150 [==============================] - 0s 55us/step - loss: 0.7167\n",
      "Epoch 249/500\n",
      "150/150 [==============================] - 0s 50us/step - loss: 0.7157\n",
      "Epoch 250/500\n",
      "150/150 [==============================] - 0s 75us/step - loss: 0.7146\n",
      "Epoch 251/500\n",
      "150/150 [==============================] - 0s 95us/step - loss: 0.7135\n",
      "Epoch 252/500\n",
      "150/150 [==============================] - 0s 71us/step - loss: 0.7125\n",
      "Epoch 253/500\n",
      "150/150 [==============================] - 0s 68us/step - loss: 0.7114\n",
      "Epoch 254/500\n",
      "150/150 [==============================] - 0s 78us/step - loss: 0.7103\n",
      "Epoch 255/500\n",
      "150/150 [==============================] - 0s 80us/step - loss: 0.7093\n",
      "Epoch 256/500\n",
      "150/150 [==============================] - 0s 83us/step - loss: 0.7082\n",
      "Epoch 257/500\n",
      "150/150 [==============================] - 0s 93us/step - loss: 0.7072\n",
      "Epoch 258/500\n",
      "150/150 [==============================] - 0s 98us/step - loss: 0.7062\n",
      "Epoch 259/500\n",
      "150/150 [==============================] - 0s 83us/step - loss: 0.7051\n",
      "Epoch 260/500\n",
      "150/150 [==============================] - 0s 70us/step - loss: 0.7041\n",
      "Epoch 261/500\n",
      "150/150 [==============================] - 0s 53us/step - loss: 0.7031\n",
      "Epoch 262/500\n",
      "150/150 [==============================] - 0s 88us/step - loss: 0.7021\n",
      "Epoch 263/500\n",
      "150/150 [==============================] - 0s 63us/step - loss: 0.7010\n",
      "Epoch 264/500\n",
      "150/150 [==============================] - 0s 76us/step - loss: 0.7000\n",
      "Epoch 265/500\n",
      "150/150 [==============================] - 0s 70us/step - loss: 0.6990\n",
      "Epoch 266/500\n",
      "150/150 [==============================] - 0s 87us/step - loss: 0.6980\n",
      "Epoch 267/500\n",
      "150/150 [==============================] - 0s 59us/step - loss: 0.6970\n",
      "Epoch 268/500\n",
      "150/150 [==============================] - 0s 80us/step - loss: 0.6960\n",
      "Epoch 269/500\n",
      "150/150 [==============================] - 0s 61us/step - loss: 0.6950\n",
      "Epoch 270/500\n",
      "150/150 [==============================] - 0s 71us/step - loss: 0.6940\n",
      "Epoch 271/500\n",
      "150/150 [==============================] - 0s 82us/step - loss: 0.6930\n",
      "Epoch 272/500\n",
      "150/150 [==============================] - 0s 87us/step - loss: 0.6920\n",
      "Epoch 273/500\n",
      "150/150 [==============================] - 0s 79us/step - loss: 0.6910\n",
      "Epoch 274/500\n",
      "150/150 [==============================] - 0s 83us/step - loss: 0.6901\n",
      "Epoch 275/500\n",
      "150/150 [==============================] - 0s 75us/step - loss: 0.6891\n",
      "Epoch 276/500\n",
      "150/150 [==============================] - 0s 68us/step - loss: 0.6881\n",
      "Epoch 277/500\n",
      "150/150 [==============================] - 0s 83us/step - loss: 0.6871\n",
      "Epoch 278/500\n",
      "150/150 [==============================] - 0s 76us/step - loss: 0.6862\n",
      "Epoch 279/500\n",
      "150/150 [==============================] - 0s 90us/step - loss: 0.6852\n",
      "Epoch 280/500\n",
      "150/150 [==============================] - 0s 73us/step - loss: 0.6842\n",
      "Epoch 281/500\n",
      "150/150 [==============================] - 0s 92us/step - loss: 0.6833\n",
      "Epoch 282/500\n",
      "150/150 [==============================] - 0s 73us/step - loss: 0.6823\n",
      "Epoch 283/500\n",
      "150/150 [==============================] - 0s 77us/step - loss: 0.6814\n",
      "Epoch 284/500\n",
      "150/150 [==============================] - 0s 63us/step - loss: 0.6804\n",
      "Epoch 285/500\n",
      "150/150 [==============================] - 0s 72us/step - loss: 0.6795\n",
      "Epoch 286/500\n",
      "150/150 [==============================] - 0s 80us/step - loss: 0.6785\n",
      "Epoch 287/500\n",
      "150/150 [==============================] - 0s 90us/step - loss: 0.6776\n",
      "Epoch 288/500\n",
      "150/150 [==============================] - 0s 52us/step - loss: 0.6767\n",
      "Epoch 289/500\n",
      "150/150 [==============================] - 0s 51us/step - loss: 0.6757\n",
      "Epoch 290/500\n",
      "150/150 [==============================] - 0s 77us/step - loss: 0.6748\n",
      "Epoch 291/500\n",
      "150/150 [==============================] - 0s 76us/step - loss: 0.6739\n",
      "Epoch 292/500\n",
      "150/150 [==============================] - 0s 62us/step - loss: 0.6729\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 293/500\n",
      "150/150 [==============================] - 0s 74us/step - loss: 0.6720\n",
      "Epoch 294/500\n",
      "150/150 [==============================] - 0s 61us/step - loss: 0.6711\n",
      "Epoch 295/500\n",
      "150/150 [==============================] - 0s 50us/step - loss: 0.6702\n",
      "Epoch 296/500\n",
      "150/150 [==============================] - 0s 67us/step - loss: 0.6693\n",
      "Epoch 297/500\n",
      "150/150 [==============================] - 0s 108us/step - loss: 0.6683\n",
      "Epoch 298/500\n",
      "150/150 [==============================] - 0s 60us/step - loss: 0.6674\n",
      "Epoch 299/500\n",
      "150/150 [==============================] - 0s 79us/step - loss: 0.6665\n",
      "Epoch 300/500\n",
      "150/150 [==============================] - 0s 79us/step - loss: 0.6656\n",
      "Epoch 301/500\n",
      "150/150 [==============================] - 0s 74us/step - loss: 0.6647\n",
      "Epoch 302/500\n",
      "150/150 [==============================] - 0s 54us/step - loss: 0.6638\n",
      "Epoch 303/500\n",
      "150/150 [==============================] - 0s 61us/step - loss: 0.6629\n",
      "Epoch 304/500\n",
      "150/150 [==============================] - 0s 74us/step - loss: 0.6620\n",
      "Epoch 305/500\n",
      "150/150 [==============================] - 0s 59us/step - loss: 0.6611\n",
      "Epoch 306/500\n",
      "150/150 [==============================] - 0s 87us/step - loss: 0.6602\n",
      "Epoch 307/500\n",
      "150/150 [==============================] - 0s 65us/step - loss: 0.6594\n",
      "Epoch 308/500\n",
      "150/150 [==============================] - 0s 84us/step - loss: 0.6585\n",
      "Epoch 309/500\n",
      "150/150 [==============================] - 0s 67us/step - loss: 0.6576\n",
      "Epoch 310/500\n",
      "150/150 [==============================] - 0s 70us/step - loss: 0.6567\n",
      "Epoch 311/500\n",
      "150/150 [==============================] - 0s 75us/step - loss: 0.6558\n",
      "Epoch 312/500\n",
      "150/150 [==============================] - 0s 63us/step - loss: 0.6549\n",
      "Epoch 313/500\n",
      "150/150 [==============================] - 0s 89us/step - loss: 0.6541\n",
      "Epoch 314/500\n",
      "150/150 [==============================] - 0s 65us/step - loss: 0.6532\n",
      "Epoch 315/500\n",
      "150/150 [==============================] - 0s 73us/step - loss: 0.6523\n",
      "Epoch 316/500\n",
      "150/150 [==============================] - 0s 59us/step - loss: 0.6515\n",
      "Epoch 317/500\n",
      "150/150 [==============================] - 0s 81us/step - loss: 0.6506\n",
      "Epoch 318/500\n",
      "150/150 [==============================] - 0s 85us/step - loss: 0.6497\n",
      "Epoch 319/500\n",
      "150/150 [==============================] - 0s 62us/step - loss: 0.6489\n",
      "Epoch 320/500\n",
      "150/150 [==============================] - 0s 75us/step - loss: 0.6480\n",
      "Epoch 321/500\n",
      "150/150 [==============================] - 0s 54us/step - loss: 0.6471\n",
      "Epoch 322/500\n",
      "150/150 [==============================] - 0s 56us/step - loss: 0.6463\n",
      "Epoch 323/500\n",
      "150/150 [==============================] - 0s 82us/step - loss: 0.6454\n",
      "Epoch 324/500\n",
      "150/150 [==============================] - 0s 66us/step - loss: 0.6446\n",
      "Epoch 325/500\n",
      "150/150 [==============================] - 0s 75us/step - loss: 0.6437\n",
      "Epoch 326/500\n",
      "150/150 [==============================] - 0s 82us/step - loss: 0.6429\n",
      "Epoch 327/500\n",
      "150/150 [==============================] - 0s 66us/step - loss: 0.6420\n",
      "Epoch 328/500\n",
      "150/150 [==============================] - 0s 77us/step - loss: 0.6412\n",
      "Epoch 329/500\n",
      "150/150 [==============================] - 0s 66us/step - loss: 0.6403\n",
      "Epoch 330/500\n",
      "150/150 [==============================] - 0s 63us/step - loss: 0.6395\n",
      "Epoch 331/500\n",
      "150/150 [==============================] - 0s 68us/step - loss: 0.6386\n",
      "Epoch 332/500\n",
      "150/150 [==============================] - 0s 71us/step - loss: 0.6378\n",
      "Epoch 333/500\n",
      "150/150 [==============================] - 0s 62us/step - loss: 0.6370\n",
      "Epoch 334/500\n",
      "150/150 [==============================] - 0s 59us/step - loss: 0.6361\n",
      "Epoch 335/500\n",
      "150/150 [==============================] - 0s 62us/step - loss: 0.6353\n",
      "Epoch 336/500\n",
      "150/150 [==============================] - 0s 67us/step - loss: 0.6345\n",
      "Epoch 337/500\n",
      "150/150 [==============================] - 0s 59us/step - loss: 0.6336\n",
      "Epoch 338/500\n",
      "150/150 [==============================] - 0s 75us/step - loss: 0.6328\n",
      "Epoch 339/500\n",
      "150/150 [==============================] - 0s 85us/step - loss: 0.6320\n",
      "Epoch 340/500\n",
      "150/150 [==============================] - 0s 70us/step - loss: 0.6312\n",
      "Epoch 341/500\n",
      "150/150 [==============================] - 0s 96us/step - loss: 0.6303\n",
      "Epoch 342/500\n",
      "150/150 [==============================] - 0s 86us/step - loss: 0.6295\n",
      "Epoch 343/500\n",
      "150/150 [==============================] - 0s 82us/step - loss: 0.6287\n",
      "Epoch 344/500\n",
      "150/150 [==============================] - 0s 57us/step - loss: 0.6279\n",
      "Epoch 345/500\n",
      "150/150 [==============================] - 0s 77us/step - loss: 0.6271\n",
      "Epoch 346/500\n",
      "150/150 [==============================] - 0s 59us/step - loss: 0.6262\n",
      "Epoch 347/500\n",
      "150/150 [==============================] - 0s 74us/step - loss: 0.6254\n",
      "Epoch 348/500\n",
      "150/150 [==============================] - 0s 76us/step - loss: 0.6246\n",
      "Epoch 349/500\n",
      "150/150 [==============================] - 0s 63us/step - loss: 0.6238\n",
      "Epoch 350/500\n",
      "150/150 [==============================] - 0s 64us/step - loss: 0.6230\n",
      "Epoch 351/500\n",
      "150/150 [==============================] - 0s 65us/step - loss: 0.6222\n",
      "Epoch 352/500\n",
      "150/150 [==============================] - 0s 85us/step - loss: 0.6214\n",
      "Epoch 353/500\n",
      "150/150 [==============================] - 0s 73us/step - loss: 0.6206\n",
      "Epoch 354/500\n",
      "150/150 [==============================] - 0s 60us/step - loss: 0.6198\n",
      "Epoch 355/500\n",
      "150/150 [==============================] - 0s 52us/step - loss: 0.6191\n",
      "Epoch 356/500\n",
      "150/150 [==============================] - 0s 67us/step - loss: 0.6183\n",
      "Epoch 357/500\n",
      "150/150 [==============================] - 0s 62us/step - loss: 0.6175\n",
      "Epoch 358/500\n",
      "150/150 [==============================] - 0s 58us/step - loss: 0.6167\n",
      "Epoch 359/500\n",
      "150/150 [==============================] - 0s 75us/step - loss: 0.6159\n",
      "Epoch 360/500\n",
      "150/150 [==============================] - 0s 78us/step - loss: 0.6151\n",
      "Epoch 361/500\n",
      "150/150 [==============================] - 0s 70us/step - loss: 0.6144\n",
      "Epoch 362/500\n",
      "150/150 [==============================] - 0s 67us/step - loss: 0.6136\n",
      "Epoch 363/500\n",
      "150/150 [==============================] - 0s 69us/step - loss: 0.6128\n",
      "Epoch 364/500\n",
      "150/150 [==============================] - 0s 68us/step - loss: 0.6120\n",
      "Epoch 365/500\n",
      "150/150 [==============================] - 0s 77us/step - loss: 0.6113\n",
      "Epoch 366/500\n",
      "150/150 [==============================] - 0s 52us/step - loss: 0.6105\n",
      "Epoch 367/500\n",
      "150/150 [==============================] - 0s 56us/step - loss: 0.6097\n",
      "Epoch 368/500\n",
      "150/150 [==============================] - 0s 63us/step - loss: 0.6090\n",
      "Epoch 369/500\n",
      "150/150 [==============================] - 0s 66us/step - loss: 0.6082\n",
      "Epoch 370/500\n",
      "150/150 [==============================] - 0s 68us/step - loss: 0.6074\n",
      "Epoch 371/500\n",
      "150/150 [==============================] - 0s 74us/step - loss: 0.6067\n",
      "Epoch 372/500\n",
      "150/150 [==============================] - 0s 90us/step - loss: 0.6059\n",
      "Epoch 373/500\n",
      "150/150 [==============================] - 0s 75us/step - loss: 0.6051\n",
      "Epoch 374/500\n",
      "150/150 [==============================] - 0s 73us/step - loss: 0.6044\n",
      "Epoch 375/500\n",
      "150/150 [==============================] - 0s 83us/step - loss: 0.6036\n",
      "Epoch 376/500\n",
      "150/150 [==============================] - 0s 87us/step - loss: 0.6029\n",
      "Epoch 377/500\n",
      "150/150 [==============================] - 0s 55us/step - loss: 0.6021\n",
      "Epoch 378/500\n",
      "150/150 [==============================] - 0s 77us/step - loss: 0.6013\n",
      "Epoch 379/500\n",
      "150/150 [==============================] - 0s 78us/step - loss: 0.6006\n",
      "Epoch 380/500\n",
      "150/150 [==============================] - 0s 86us/step - loss: 0.5998\n",
      "Epoch 381/500\n",
      "150/150 [==============================] - 0s 70us/step - loss: 0.5991\n",
      "Epoch 382/500\n",
      "150/150 [==============================] - 0s 81us/step - loss: 0.5983\n",
      "Epoch 383/500\n",
      "150/150 [==============================] - 0s 71us/step - loss: 0.5976\n",
      "Epoch 384/500\n",
      "150/150 [==============================] - 0s 76us/step - loss: 0.5969\n",
      "Epoch 385/500\n",
      "150/150 [==============================] - 0s 99us/step - loss: 0.5961\n",
      "Epoch 386/500\n",
      "150/150 [==============================] - 0s 62us/step - loss: 0.5954\n",
      "Epoch 387/500\n",
      "150/150 [==============================] - 0s 62us/step - loss: 0.5946\n",
      "Epoch 388/500\n",
      "150/150 [==============================] - 0s 79us/step - loss: 0.5939\n",
      "Epoch 389/500\n",
      "150/150 [==============================] - 0s 87us/step - loss: 0.5932\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 390/500\n",
      "150/150 [==============================] - 0s 62us/step - loss: 0.5924\n",
      "Epoch 391/500\n",
      "150/150 [==============================] - 0s 82us/step - loss: 0.5917\n",
      "Epoch 392/500\n",
      "150/150 [==============================] - 0s 81us/step - loss: 0.5909\n",
      "Epoch 393/500\n",
      "150/150 [==============================] - 0s 83us/step - loss: 0.5902\n",
      "Epoch 394/500\n",
      "150/150 [==============================] - 0s 78us/step - loss: 0.5895\n",
      "Epoch 395/500\n",
      "150/150 [==============================] - 0s 70us/step - loss: 0.5887\n",
      "Epoch 396/500\n",
      "150/150 [==============================] - 0s 76us/step - loss: 0.5880\n",
      "Epoch 397/500\n",
      "150/150 [==============================] - 0s 82us/step - loss: 0.5873\n",
      "Epoch 398/500\n",
      "150/150 [==============================] - 0s 73us/step - loss: 0.5865\n",
      "Epoch 399/500\n",
      "150/150 [==============================] - 0s 62us/step - loss: 0.5858\n",
      "Epoch 400/500\n",
      "150/150 [==============================] - 0s 69us/step - loss: 0.5851\n",
      "Epoch 401/500\n",
      "150/150 [==============================] - 0s 86us/step - loss: 0.5844\n",
      "Epoch 402/500\n",
      "150/150 [==============================] - 0s 67us/step - loss: 0.5836\n",
      "Epoch 403/500\n",
      "150/150 [==============================] - 0s 65us/step - loss: 0.5829\n",
      "Epoch 404/500\n",
      "150/150 [==============================] - 0s 66us/step - loss: 0.5822\n",
      "Epoch 405/500\n",
      "150/150 [==============================] - 0s 61us/step - loss: 0.5815\n",
      "Epoch 406/500\n",
      "150/150 [==============================] - 0s 91us/step - loss: 0.5808\n",
      "Epoch 407/500\n",
      "150/150 [==============================] - 0s 72us/step - loss: 0.5801\n",
      "Epoch 408/500\n",
      "150/150 [==============================] - 0s 98us/step - loss: 0.5794\n",
      "Epoch 409/500\n",
      "150/150 [==============================] - 0s 85us/step - loss: 0.5787\n",
      "Epoch 410/500\n",
      "150/150 [==============================] - 0s 89us/step - loss: 0.5780\n",
      "Epoch 411/500\n",
      "150/150 [==============================] - 0s 90us/step - loss: 0.5772\n",
      "Epoch 412/500\n",
      "150/150 [==============================] - 0s 69us/step - loss: 0.5765\n",
      "Epoch 413/500\n",
      "150/150 [==============================] - 0s 76us/step - loss: 0.5758\n",
      "Epoch 414/500\n",
      "150/150 [==============================] - 0s 70us/step - loss: 0.5751\n",
      "Epoch 415/500\n",
      "150/150 [==============================] - 0s 86us/step - loss: 0.5744\n",
      "Epoch 416/500\n",
      "150/150 [==============================] - 0s 76us/step - loss: 0.5737\n",
      "Epoch 417/500\n",
      "150/150 [==============================] - 0s 55us/step - loss: 0.5730\n",
      "Epoch 418/500\n",
      "150/150 [==============================] - 0s 68us/step - loss: 0.5723\n",
      "Epoch 419/500\n",
      "150/150 [==============================] - 0s 73us/step - loss: 0.5716\n",
      "Epoch 420/500\n",
      "150/150 [==============================] - 0s 62us/step - loss: 0.5709\n",
      "Epoch 421/500\n",
      "150/150 [==============================] - 0s 72us/step - loss: 0.5702\n",
      "Epoch 422/500\n",
      "150/150 [==============================] - 0s 69us/step - loss: 0.5696\n",
      "Epoch 423/500\n",
      "150/150 [==============================] - 0s 77us/step - loss: 0.5689\n",
      "Epoch 424/500\n",
      "150/150 [==============================] - 0s 71us/step - loss: 0.5682\n",
      "Epoch 425/500\n",
      "150/150 [==============================] - 0s 88us/step - loss: 0.5675\n",
      "Epoch 426/500\n",
      "150/150 [==============================] - 0s 89us/step - loss: 0.5668\n",
      "Epoch 427/500\n",
      "150/150 [==============================] - 0s 69us/step - loss: 0.5661\n",
      "Epoch 428/500\n",
      "150/150 [==============================] - 0s 87us/step - loss: 0.5654\n",
      "Epoch 429/500\n",
      "150/150 [==============================] - 0s 107us/step - loss: 0.5647\n",
      "Epoch 430/500\n",
      "150/150 [==============================] - 0s 76us/step - loss: 0.5640\n",
      "Epoch 431/500\n",
      "150/150 [==============================] - 0s 81us/step - loss: 0.5633\n",
      "Epoch 432/500\n",
      "150/150 [==============================] - 0s 83us/step - loss: 0.5626\n",
      "Epoch 433/500\n",
      "150/150 [==============================] - 0s 69us/step - loss: 0.5619\n",
      "Epoch 434/500\n",
      "150/150 [==============================] - 0s 81us/step - loss: 0.5613\n",
      "Epoch 435/500\n",
      "150/150 [==============================] - 0s 80us/step - loss: 0.5606\n",
      "Epoch 436/500\n",
      "150/150 [==============================] - 0s 54us/step - loss: 0.5599\n",
      "Epoch 437/500\n",
      "150/150 [==============================] - 0s 87us/step - loss: 0.5592\n",
      "Epoch 438/500\n",
      "150/150 [==============================] - 0s 57us/step - loss: 0.5585\n",
      "Epoch 439/500\n",
      "150/150 [==============================] - 0s 77us/step - loss: 0.5578\n",
      "Epoch 440/500\n",
      "150/150 [==============================] - 0s 61us/step - loss: 0.5572\n",
      "Epoch 441/500\n",
      "150/150 [==============================] - 0s 57us/step - loss: 0.5565\n",
      "Epoch 442/500\n",
      "150/150 [==============================] - 0s 59us/step - loss: 0.5558\n",
      "Epoch 443/500\n",
      "150/150 [==============================] - 0s 80us/step - loss: 0.5551\n",
      "Epoch 444/500\n",
      "150/150 [==============================] - 0s 104us/step - loss: 0.5545\n",
      "Epoch 445/500\n",
      "150/150 [==============================] - 0s 99us/step - loss: 0.5538\n",
      "Epoch 446/500\n",
      "150/150 [==============================] - 0s 86us/step - loss: 0.5531\n",
      "Epoch 447/500\n",
      "150/150 [==============================] - 0s 80us/step - loss: 0.5525\n",
      "Epoch 448/500\n",
      "150/150 [==============================] - 0s 86us/step - loss: 0.5518\n",
      "Epoch 449/500\n",
      "150/150 [==============================] - 0s 84us/step - loss: 0.5512\n",
      "Epoch 450/500\n",
      "150/150 [==============================] - 0s 80us/step - loss: 0.5505\n",
      "Epoch 451/500\n",
      "150/150 [==============================] - 0s 82us/step - loss: 0.5499\n",
      "Epoch 452/500\n",
      "150/150 [==============================] - 0s 90us/step - loss: 0.5492\n",
      "Epoch 453/500\n",
      "150/150 [==============================] - 0s 88us/step - loss: 0.5486\n",
      "Epoch 454/500\n",
      "150/150 [==============================] - 0s 80us/step - loss: 0.5479\n",
      "Epoch 455/500\n",
      "150/150 [==============================] - 0s 74us/step - loss: 0.5473\n",
      "Epoch 456/500\n",
      "150/150 [==============================] - 0s 81us/step - loss: 0.5466\n",
      "Epoch 457/500\n",
      "150/150 [==============================] - 0s 81us/step - loss: 0.5460\n",
      "Epoch 458/500\n",
      "150/150 [==============================] - 0s 83us/step - loss: 0.5453\n",
      "Epoch 459/500\n",
      "150/150 [==============================] - 0s 99us/step - loss: 0.5447\n",
      "Epoch 460/500\n",
      "150/150 [==============================] - 0s 91us/step - loss: 0.5440\n",
      "Epoch 461/500\n",
      "150/150 [==============================] - 0s 76us/step - loss: 0.5434\n",
      "Epoch 462/500\n",
      "150/150 [==============================] - 0s 73us/step - loss: 0.5428\n",
      "Epoch 463/500\n",
      "150/150 [==============================] - 0s 82us/step - loss: 0.5421\n",
      "Epoch 464/500\n",
      "150/150 [==============================] - 0s 71us/step - loss: 0.5415\n",
      "Epoch 465/500\n",
      "150/150 [==============================] - 0s 74us/step - loss: 0.5409\n",
      "Epoch 466/500\n",
      "150/150 [==============================] - 0s 76us/step - loss: 0.5402\n",
      "Epoch 467/500\n",
      "150/150 [==============================] - 0s 83us/step - loss: 0.5396\n",
      "Epoch 468/500\n",
      "150/150 [==============================] - 0s 83us/step - loss: 0.5390\n",
      "Epoch 469/500\n",
      "150/150 [==============================] - 0s 72us/step - loss: 0.5384\n",
      "Epoch 470/500\n",
      "150/150 [==============================] - 0s 66us/step - loss: 0.5377\n",
      "Epoch 471/500\n",
      "150/150 [==============================] - 0s 71us/step - loss: 0.5371\n",
      "Epoch 472/500\n",
      "150/150 [==============================] - 0s 61us/step - loss: 0.5365\n",
      "Epoch 473/500\n",
      "150/150 [==============================] - 0s 53us/step - loss: 0.5359\n",
      "Epoch 474/500\n",
      "150/150 [==============================] - 0s 68us/step - loss: 0.5352\n",
      "Epoch 475/500\n",
      "150/150 [==============================] - 0s 93us/step - loss: 0.5346\n",
      "Epoch 476/500\n",
      "150/150 [==============================] - 0s 81us/step - loss: 0.5340\n",
      "Epoch 477/500\n",
      "150/150 [==============================] - 0s 58us/step - loss: 0.5334\n",
      "Epoch 478/500\n",
      "150/150 [==============================] - 0s 68us/step - loss: 0.5327\n",
      "Epoch 479/500\n",
      "150/150 [==============================] - 0s 64us/step - loss: 0.5321\n",
      "Epoch 480/500\n",
      "150/150 [==============================] - 0s 70us/step - loss: 0.5315\n",
      "Epoch 481/500\n",
      "150/150 [==============================] - 0s 71us/step - loss: 0.5309\n",
      "Epoch 482/500\n",
      "150/150 [==============================] - 0s 67us/step - loss: 0.5303\n",
      "Epoch 483/500\n",
      "150/150 [==============================] - 0s 71us/step - loss: 0.5297\n",
      "Epoch 484/500\n",
      "150/150 [==============================] - 0s 82us/step - loss: 0.5291\n",
      "Epoch 485/500\n",
      "150/150 [==============================] - 0s 61us/step - loss: 0.5284\n",
      "Epoch 486/500\n",
      "150/150 [==============================] - 0s 73us/step - loss: 0.5278\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 487/500\n",
      "150/150 [==============================] - 0s 80us/step - loss: 0.5272\n",
      "Epoch 488/500\n",
      "150/150 [==============================] - 0s 79us/step - loss: 0.5266\n",
      "Epoch 489/500\n",
      "150/150 [==============================] - 0s 73us/step - loss: 0.5260\n",
      "Epoch 490/500\n",
      "150/150 [==============================] - 0s 71us/step - loss: 0.5254\n",
      "Epoch 491/500\n",
      "150/150 [==============================] - 0s 67us/step - loss: 0.5248\n",
      "Epoch 492/500\n",
      "150/150 [==============================] - 0s 89us/step - loss: 0.5242\n",
      "Epoch 493/500\n",
      "150/150 [==============================] - 0s 89us/step - loss: 0.5236\n",
      "Epoch 494/500\n",
      "150/150 [==============================] - 0s 69us/step - loss: 0.5230\n",
      "Epoch 495/500\n",
      "150/150 [==============================] - 0s 61us/step - loss: 0.5224\n",
      "Epoch 496/500\n",
      "150/150 [==============================] - 0s 83us/step - loss: 0.5218\n",
      "Epoch 497/500\n",
      "150/150 [==============================] - 0s 84us/step - loss: 0.5212\n",
      "Epoch 498/500\n",
      "150/150 [==============================] - 0s 77us/step - loss: 0.5206\n",
      "Epoch 499/500\n",
      "150/150 [==============================] - 0s 68us/step - loss: 0.5200\n",
      "Epoch 500/500\n",
      "150/150 [==============================] - 0s 70us/step - loss: 0.5195\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(x,y,batch_size=150,epochs=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一开始loss非常大，而随着训练不断的进行，loss在逐渐减小。将history中的数据通过matplotlib绘图表现出来，就非常直观了。因为在jupyter上调试，我加入%matplotlib inline命令。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f6f883e48>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#将图片内嵌在交互窗口，而不是弹出一个图片窗口\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(500),history.history['loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们如何评价一个模型训练是否成功？首先，训练过程中训练集loss要下降到一个较小的值，表示模型收敛较好，没有欠拟合；其次，测试集loss最后与训练集loss要尽可能相似，差距越小越好小，说明该模型没有过拟合。\n",
    "\n",
    "模型成功训练出来后，便可以使用该模型对输入的鸢尾花数据，判断是属于哪一种类别了。这里选择一个最简单的部分，随机读取数据集中的几条数据，看看这个模型会输出什么结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以在数据集中随机选择几条，略作修改后进行测试。看看输出的结果对不对。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sepal.Length</th>\n",
       "      <th>Sepal.Width</th>\n",
       "      <th>Petal.Length</th>\n",
       "      <th>Petal.Width</th>\n",
       "      <th>Species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.5</td>\n",
       "      <td>2.6</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.2</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.1</td>\n",
       "      <td>2.6</td>\n",
       "      <td>5.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.8</td>\n",
       "      <td>1.6</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sepal.Length  Sepal.Width  Petal.Length  Petal.Width          Species\n",
       "0           4.6          3.1           1.5          0.2      Iris-setosa\n",
       "1           5.5          2.6           4.4          1.2  Iris-versicolor\n",
       "2           6.1          2.6           5.6          1.4   Iris-virginica\n",
       "3           7.2          3.0           5.8          1.6   Iris-virginica\n",
       "4           5.2          3.4           1.4          0.2      Iris-setosa"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv('./data/iris.csv')\n",
    "data=data.sample(frac=1).reset_index(drop=True)   #打乱数据的先后顺序\n",
    "x_input=data.iloc[:,0:-1]\n",
    "x_input=x_input[:5]\n",
    "data[:5].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.81645006, 0.08945613, 0.09409384],\n",
       "       [0.05531117, 0.4335839 , 0.51110494],\n",
       "       [0.01924712, 0.40849388, 0.57225895],\n",
       "       [0.03210969, 0.40018922, 0.56770104],\n",
       "       [0.9145784 , 0.04051452, 0.04490715]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(x_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里的数据是按照“Species_Iris-setosa，Species_Iris-versicolor，Species_Iris-virginica”来排序的。模型输出的数据中，每一列都是估算，哪一列数据大，我们就选择哪一个分类结果。\n",
    "\n",
    "如果发现不准确，我们继续训练一下模型，即再运行几次，等到loss的值没有显著变化的时候，再来测试模型。一般来说，只要loss值到达0.1左右，识别效果就很不错了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.利用自带数据集\n",
    "\n",
    "参考资料：https://keras.io/zh/\n",
    "\n",
    "**5.1.数据导入**\n",
    "\n",
    "直接使用sklearn自带的数据集，即导入sklearn.datasets。貌似要从网络下载数据的，第一次使用，要等一会儿。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "['setosa' 'versicolor' 'virginica']\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import layers\n",
    "from sklearn.datasets import load_iris\n",
    "i_data = load_iris()\n",
    "print(i_data.feature_names)\n",
    "print(i_data.target_names)\n",
    "x = i_data.data\n",
    "y = i_data.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.2.数据预处理**\n",
    "\n",
    "Scikit-Learn已经帮我们把类别编码成了数字，不过是一维数组（None, ）（样本的个数不固定，用None表示），而Keras多分类接受的类别输入是一个二维数组，是y的one-hot编码形式。one-hot编码，简单来讲，就是将原来由0开始的类别值转换成向量，比如3个类别0,1,2，那么类别向量长度为3，以原类别值作为位置索引，对应位置置为1，其它位置置为0，即类别0对应：[1, 0, 0]，类别1对应[0, 1, 0]，类别2对应[0, 0, 1]。全部转换后，y变为二维数组（None，3），可以打印前3行看看。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 3)\n",
      "[[1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "y = to_categorical(y, 3)\n",
    "print(y.shape)\n",
    "print(y[0:3, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.3.定义模型（搭建神经网络）**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "model = Sequential()\n",
    "from keras.layers import Dense\n",
    "model.add(Dense(units=8, input_dim=4, activation='relu'))\n",
    "model.add(Dense(units=3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.4.编译模型**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.5.训练模型**\n",
    "\n",
    "这段代码可以多运行几次，你会发现loss多值会越来越小。到了0.05后，变化就不大了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.3501 - acc: 0.9600\n",
      "Epoch 2/200\n",
      "150/150 [==============================] - 0s 974us/step - loss: 0.3497 - acc: 0.9667\n",
      "Epoch 3/200\n",
      "150/150 [==============================] - 0s 916us/step - loss: 0.3467 - acc: 0.9600\n",
      "Epoch 4/200\n",
      "150/150 [==============================] - 0s 874us/step - loss: 0.3460 - acc: 0.9667\n",
      "Epoch 5/200\n",
      "150/150 [==============================] - 0s 979us/step - loss: 0.3451 - acc: 0.9467\n",
      "Epoch 6/200\n",
      "150/150 [==============================] - 0s 994us/step - loss: 0.3427 - acc: 0.9533\n",
      "Epoch 7/200\n",
      "150/150 [==============================] - 0s 952us/step - loss: 0.3405 - acc: 0.9600 0s - loss: 0.3139 - acc: 0.966\n",
      "Epoch 8/200\n",
      "150/150 [==============================] - 0s 931us/step - loss: 0.3384 - acc: 0.9600\n",
      "Epoch 9/200\n",
      "150/150 [==============================] - 0s 980us/step - loss: 0.3368 - acc: 0.9733\n",
      "Epoch 10/200\n",
      "150/150 [==============================] - 0s 907us/step - loss: 0.3352 - acc: 0.9600 0s - loss: 0.3253 - acc: 0.92\n",
      "Epoch 11/200\n",
      "150/150 [==============================] - 0s 908us/step - loss: 0.3335 - acc: 0.9600\n",
      "Epoch 12/200\n",
      "150/150 [==============================] - 0s 876us/step - loss: 0.3320 - acc: 0.9667\n",
      "Epoch 13/200\n",
      "150/150 [==============================] - 0s 929us/step - loss: 0.3300 - acc: 0.9667\n",
      "Epoch 14/200\n",
      "150/150 [==============================] - 0s 956us/step - loss: 0.3284 - acc: 0.9667\n",
      "Epoch 15/200\n",
      "150/150 [==============================] - 0s 990us/step - loss: 0.3280 - acc: 0.9667\n",
      "Epoch 16/200\n",
      "150/150 [==============================] - 0s 980us/step - loss: 0.3255 - acc: 0.9600\n",
      "Epoch 17/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.3236 - acc: 0.9600\n",
      "Epoch 18/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.3220 - acc: 0.9733\n",
      "Epoch 19/200\n",
      "150/150 [==============================] - 0s 807us/step - loss: 0.3201 - acc: 0.9733\n",
      "Epoch 20/200\n",
      "150/150 [==============================] - 0s 901us/step - loss: 0.3188 - acc: 0.9667\n",
      "Epoch 21/200\n",
      "150/150 [==============================] - 0s 868us/step - loss: 0.3172 - acc: 0.9733 0s - loss: 0.3192 - acc: 0.98\n",
      "Epoch 22/200\n",
      "150/150 [==============================] - 0s 870us/step - loss: 0.3175 - acc: 0.9800\n",
      "Epoch 23/200\n",
      "150/150 [==============================] - 0s 884us/step - loss: 0.3146 - acc: 0.9667\n",
      "Epoch 24/200\n",
      "150/150 [==============================] - 0s 842us/step - loss: 0.3130 - acc: 0.9600\n",
      "Epoch 25/200\n",
      "150/150 [==============================] - 0s 915us/step - loss: 0.3104 - acc: 0.9600\n",
      "Epoch 26/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.3084 - acc: 0.9733\n",
      "Epoch 27/200\n",
      "150/150 [==============================] - 0s 946us/step - loss: 0.3074 - acc: 0.9733\n",
      "Epoch 28/200\n",
      "150/150 [==============================] - 0s 993us/step - loss: 0.3052 - acc: 0.9733\n",
      "Epoch 29/200\n",
      "150/150 [==============================] - 0s 883us/step - loss: 0.3038 - acc: 0.9733\n",
      "Epoch 30/200\n",
      "150/150 [==============================] - 0s 900us/step - loss: 0.3023 - acc: 0.9733 0s - loss: 0.3035 - acc: 0.971\n",
      "Epoch 31/200\n",
      "150/150 [==============================] - 0s 840us/step - loss: 0.3010 - acc: 0.9733\n",
      "Epoch 32/200\n",
      "150/150 [==============================] - 0s 923us/step - loss: 0.2985 - acc: 0.9733\n",
      "Epoch 33/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.2987 - acc: 0.9733\n",
      "Epoch 34/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.2967 - acc: 0.9733\n",
      "Epoch 35/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.2942 - acc: 0.9733\n",
      "Epoch 36/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.2923 - acc: 0.9733A: 0s - loss: 0.2950 - acc: 0.95\n",
      "Epoch 37/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.2913 - acc: 0.9733\n",
      "Epoch 38/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.2922 - acc: 0.9600\n",
      "Epoch 39/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.2887 - acc: 0.9800\n",
      "Epoch 40/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.2863 - acc: 0.9800A: 0s - loss: 0.2743 - acc: 0.990\n",
      "Epoch 41/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.2848 - acc: 0.9800\n",
      "Epoch 42/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.2832 - acc: 0.9733\n",
      "Epoch 43/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.2813 - acc: 0.9733\n",
      "Epoch 44/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.2802 - acc: 0.9733A: 0s - loss: 0.2871 - acc: 1.00\n",
      "Epoch 45/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.2780 - acc: 0.9733\n",
      "Epoch 46/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.2766 - acc: 0.9800\n",
      "Epoch 47/200\n",
      "150/150 [==============================] - 0s 909us/step - loss: 0.2752 - acc: 0.9800\n",
      "Epoch 48/200\n",
      "150/150 [==============================] - 0s 953us/step - loss: 0.2736 - acc: 0.9800\n",
      "Epoch 49/200\n",
      "150/150 [==============================] - 0s 944us/step - loss: 0.2720 - acc: 0.9800\n",
      "Epoch 50/200\n",
      "150/150 [==============================] - 0s 966us/step - loss: 0.2706 - acc: 0.9733\n",
      "Epoch 51/200\n",
      "150/150 [==============================] - 0s 875us/step - loss: 0.2698 - acc: 0.9733\n",
      "Epoch 52/200\n",
      "150/150 [==============================] - 0s 863us/step - loss: 0.2688 - acc: 0.9800\n",
      "Epoch 53/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.2657 - acc: 0.9800\n",
      "Epoch 54/200\n",
      "150/150 [==============================] - 0s 843us/step - loss: 0.2647 - acc: 0.9733\n",
      "Epoch 55/200\n",
      "150/150 [==============================] - 0s 892us/step - loss: 0.2633 - acc: 0.9800\n",
      "Epoch 56/200\n",
      "150/150 [==============================] - 0s 894us/step - loss: 0.2612 - acc: 0.9800\n",
      "Epoch 57/200\n",
      "150/150 [==============================] - 0s 928us/step - loss: 0.2603 - acc: 0.9800\n",
      "Epoch 58/200\n",
      "150/150 [==============================] - 0s 922us/step - loss: 0.2586 - acc: 0.9800\n",
      "Epoch 59/200\n",
      "150/150 [==============================] - 0s 918us/step - loss: 0.2573 - acc: 0.9800\n",
      "Epoch 60/200\n",
      "150/150 [==============================] - 0s 842us/step - loss: 0.2561 - acc: 0.9800\n",
      "Epoch 61/200\n",
      "150/150 [==============================] - 0s 852us/step - loss: 0.2546 - acc: 0.9800 0s - loss: 0.2505 - acc: 0.95\n",
      "Epoch 62/200\n",
      "150/150 [==============================] - 0s 933us/step - loss: 0.2537 - acc: 0.9733\n",
      "Epoch 63/200\n",
      "150/150 [==============================] - 0s 899us/step - loss: 0.2521 - acc: 0.9800\n",
      "Epoch 64/200\n",
      "150/150 [==============================] - 0s 899us/step - loss: 0.2498 - acc: 0.9800\n",
      "Epoch 65/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.2491 - acc: 0.962 - 0s 785us/step - loss: 0.2490 - acc: 0.9733\n",
      "Epoch 66/200\n",
      "150/150 [==============================] - 0s 923us/step - loss: 0.2470 - acc: 0.9800\n",
      "Epoch 67/200\n",
      "150/150 [==============================] - 0s 960us/step - loss: 0.2456 - acc: 0.9800\n",
      "Epoch 68/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.2449 - acc: 0.9800\n",
      "Epoch 69/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.2426 - acc: 0.9800A: 0s - loss: 0.2431 - acc: 0.981\n",
      "Epoch 70/200\n",
      "150/150 [==============================] - 0s 997us/step - loss: 0.2415 - acc: 0.9800\n",
      "Epoch 71/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.2508 - acc: 0.981 - 0s 1ms/step - loss: 0.2405 - acc: 0.9800\n",
      "Epoch 72/200\n",
      "150/150 [==============================] - 0s 975us/step - loss: 0.2388 - acc: 0.9800\n",
      "Epoch 73/200\n",
      "150/150 [==============================] - 0s 953us/step - loss: 0.2379 - acc: 0.9733\n",
      "Epoch 74/200\n",
      "150/150 [==============================] - 0s 959us/step - loss: 0.2363 - acc: 0.9800\n",
      "Epoch 75/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.2345 - acc: 0.9800\n",
      "Epoch 76/200\n",
      "150/150 [==============================] - 0s 988us/step - loss: 0.2341 - acc: 0.9800\n",
      "Epoch 77/200\n",
      "150/150 [==============================] - 0s 799us/step - loss: 0.2323 - acc: 0.9800\n",
      "Epoch 78/200\n",
      "150/150 [==============================] - 0s 870us/step - loss: 0.2311 - acc: 0.9800\n",
      "Epoch 79/200\n",
      "150/150 [==============================] - 0s 964us/step - loss: 0.2302 - acc: 0.9800\n",
      "Epoch 80/200\n",
      "150/150 [==============================] - 0s 895us/step - loss: 0.2284 - acc: 0.9800\n",
      "Epoch 81/200\n",
      "150/150 [==============================] - 0s 934us/step - loss: 0.2276 - acc: 0.9800\n",
      "Epoch 82/200\n",
      "150/150 [==============================] - 0s 930us/step - loss: 0.2253 - acc: 0.9800\n",
      "Epoch 83/200\n",
      "150/150 [==============================] - 0s 932us/step - loss: 0.2242 - acc: 0.9800\n",
      "Epoch 84/200\n",
      "150/150 [==============================] - 0s 962us/step - loss: 0.2242 - acc: 0.9800 0s - loss: 0.2103 - acc: 0.98\n",
      "Epoch 85/200\n",
      "150/150 [==============================] - 0s 987us/step - loss: 0.2216 - acc: 0.9800\n",
      "Epoch 86/200\n",
      "150/150 [==============================] - 0s 992us/step - loss: 0.2209 - acc: 0.9800\n",
      "Epoch 87/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.2195 - acc: 0.9800\n",
      "Epoch 88/200\n",
      "150/150 [==============================] - 0s 954us/step - loss: 0.2186 - acc: 0.9800\n",
      "Epoch 89/200\n",
      "150/150 [==============================] - 0s 880us/step - loss: 0.2172 - acc: 0.9800\n",
      "Epoch 90/200\n",
      "150/150 [==============================] - 0s 846us/step - loss: 0.2159 - acc: 0.9800\n",
      "Epoch 91/200\n",
      "150/150 [==============================] - 0s 864us/step - loss: 0.2145 - acc: 0.9800\n",
      "Epoch 92/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.2149 - acc: 0.9800\n",
      "Epoch 93/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.2119 - acc: 0.9800\n",
      "Epoch 94/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.2109 - acc: 0.9800\n",
      "Epoch 95/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.2109 - acc: 0.9800\n",
      "Epoch 96/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.2122 - acc: 0.981 - 0s 1ms/step - loss: 0.2086 - acc: 0.9800\n",
      "Epoch 97/200\n",
      "150/150 [==============================] - 0s 975us/step - loss: 0.2079 - acc: 0.9800\n",
      "Epoch 98/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.2074 - acc: 0.9800\n",
      "Epoch 99/200\n",
      "150/150 [==============================] - 0s 984us/step - loss: 0.2052 - acc: 0.9800\n",
      "Epoch 100/200\n",
      "150/150 [==============================] - 0s 992us/step - loss: 0.2061 - acc: 0.9867 0s - loss: 0.1989 - acc: 0.991\n",
      "Epoch 101/200\n",
      "150/150 [==============================] - 0s 996us/step - loss: 0.2032 - acc: 0.9800\n",
      "Epoch 102/200\n",
      "150/150 [==============================] - 0s 988us/step - loss: 0.2027 - acc: 0.9800\n",
      "Epoch 103/200\n",
      "150/150 [==============================] - 0s 915us/step - loss: 0.2007 - acc: 0.9800\n",
      "Epoch 104/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1999 - acc: 0.9800\n",
      "Epoch 105/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1988 - acc: 0.9800\n",
      "Epoch 106/200\n",
      "150/150 [==============================] - 0s 925us/step - loss: 0.1977 - acc: 0.9800\n",
      "Epoch 107/200\n",
      "150/150 [==============================] - 0s 985us/step - loss: 0.1967 - acc: 0.9800\n",
      "Epoch 108/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1955 - acc: 0.9800A: 0s - loss: 0.1988 - acc: 0.975\n",
      "Epoch 109/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1954 - acc: 0.9800\n",
      "Epoch 110/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1946 - acc: 0.9800\n",
      "Epoch 111/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1924 - acc: 0.9800\n",
      "Epoch 112/200\n",
      "150/150 [==============================] - 0s 989us/step - loss: 0.1914 - acc: 0.9800\n",
      "Epoch 113/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1905 - acc: 0.9800\n",
      "Epoch 114/200\n",
      "150/150 [==============================] - 0s 992us/step - loss: 0.1899 - acc: 0.9800\n",
      "Epoch 115/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1896 - acc: 0.9800\n",
      "Epoch 116/200\n",
      "150/150 [==============================] - 0s 985us/step - loss: 0.1875 - acc: 0.9800\n",
      "Epoch 117/200\n",
      "150/150 [==============================] - 0s 970us/step - loss: 0.1868 - acc: 0.9800\n",
      "Epoch 118/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1854 - acc: 0.9800\n",
      "Epoch 119/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1856 - acc: 0.9800\n",
      "Epoch 120/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1842 - acc: 0.9800\n",
      "Epoch 121/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1837 - acc: 0.9800\n",
      "Epoch 122/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1847 - acc: 0.9800\n",
      "Epoch 123/200\n",
      "150/150 [==============================] - 0s 975us/step - loss: 0.1810 - acc: 0.9800\n",
      "Epoch 124/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1821 - acc: 0.9733\n",
      "Epoch 125/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1840 - acc: 0.9800\n",
      "Epoch 126/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1794 - acc: 0.9800\n",
      "Epoch 127/200\n",
      "150/150 [==============================] - 0s 946us/step - loss: 0.1775 - acc: 0.9867\n",
      "Epoch 128/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1768 - acc: 0.9800\n",
      "Epoch 129/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1761 - acc: 0.9800\n",
      "Epoch 130/200\n",
      "150/150 [==============================] - 0s 926us/step - loss: 0.1742 - acc: 0.9800\n",
      "Epoch 131/200\n",
      "150/150 [==============================] - 0s 977us/step - loss: 0.1743 - acc: 0.9800\n",
      "Epoch 132/200\n",
      "150/150 [==============================] - 0s 948us/step - loss: 0.1727 - acc: 0.9800\n",
      "Epoch 133/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1720 - acc: 0.9800\n",
      "Epoch 134/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1718 - acc: 0.9800\n",
      "Epoch 135/200\n",
      "150/150 [==============================] - 0s 961us/step - loss: 0.1718 - acc: 0.9800\n",
      "Epoch 136/200\n",
      "150/150 [==============================] - 0s 928us/step - loss: 0.1699 - acc: 0.9800\n",
      "Epoch 137/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1707 - acc: 0.9800\n",
      "Epoch 138/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.1820 - acc: 0.972 - 0s 1ms/step - loss: 0.1678 - acc: 0.9800\n",
      "Epoch 139/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1685 - acc: 0.9800\n",
      "Epoch 140/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1668 - acc: 0.9800\n",
      "Epoch 141/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1666 - acc: 0.9800\n",
      "Epoch 142/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1649 - acc: 0.9800\n",
      "Epoch 143/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1637 - acc: 0.9800\n",
      "Epoch 144/200\n",
      "150/150 [==============================] - 0s 951us/step - loss: 0.1631 - acc: 0.9800\n",
      "Epoch 145/200\n",
      "150/150 [==============================] - 0s 970us/step - loss: 0.1627 - acc: 0.9800\n",
      "Epoch 146/200\n",
      "150/150 [==============================] - 0s 986us/step - loss: 0.1623 - acc: 0.9800\n",
      "Epoch 147/200\n",
      "150/150 [==============================] - 0s 769us/step - loss: 0.1610 - acc: 0.9800\n",
      "Epoch 148/200\n",
      "150/150 [==============================] - 0s 872us/step - loss: 0.1605 - acc: 0.9800\n",
      "Epoch 149/200\n",
      "150/150 [==============================] - 0s 975us/step - loss: 0.1601 - acc: 0.9800 0s - loss: 0.1735 - acc: 1.00\n",
      "Epoch 150/200\n",
      "150/150 [==============================] - 0s 813us/step - loss: 0.1593 - acc: 0.9800\n",
      "Epoch 151/200\n",
      "150/150 [==============================] - 0s 878us/step - loss: 0.1583 - acc: 0.9800 0s - loss: 0.1651 - acc: 0.97\n",
      "Epoch 152/200\n",
      "150/150 [==============================] - 0s 928us/step - loss: 0.1572 - acc: 0.9800\n",
      "Epoch 153/200\n",
      "150/150 [==============================] - 0s 982us/step - loss: 0.1574 - acc: 0.9800\n",
      "Epoch 154/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1562 - acc: 0.9800\n",
      "Epoch 155/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1554 - acc: 0.9800\n",
      "Epoch 156/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1547 - acc: 0.9800\n",
      "Epoch 157/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1538 - acc: 0.9800\n",
      "Epoch 158/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1536 - acc: 0.9800\n",
      "Epoch 159/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1525 - acc: 0.9800\n",
      "Epoch 160/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1519 - acc: 0.9800\n",
      "Epoch 161/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1512 - acc: 0.9800\n",
      "Epoch 162/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1511 - acc: 0.9867\n",
      "Epoch 163/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1503 - acc: 0.9800\n",
      "Epoch 164/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1500 - acc: 0.9800\n",
      "Epoch 165/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.1366 - acc: 0.990 - 0s 1ms/step - loss: 0.1484 - acc: 0.9800\n",
      "Epoch 166/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1487 - acc: 0.9800\n",
      "Epoch 167/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.1618 - acc: 0.975 - 0s 987us/step - loss: 0.1479 - acc: 0.9800\n",
      "Epoch 168/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.1602 - acc: 0.972 - 0s 1ms/step - loss: 0.1471 - acc: 0.9800\n",
      "Epoch 169/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1464 - acc: 0.9800\n",
      "Epoch 170/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1456 - acc: 0.9867\n",
      "Epoch 171/200\n",
      "150/150 [==============================] - 0s 992us/step - loss: 0.1449 - acc: 0.9800\n",
      "Epoch 172/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1442 - acc: 0.9800\n",
      "Epoch 173/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1442 - acc: 0.9800\n",
      "Epoch 174/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1448 - acc: 0.9800\n",
      "Epoch 175/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1435 - acc: 0.9800\n",
      "Epoch 176/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1422 - acc: 0.9800\n",
      "Epoch 177/200\n",
      "150/150 [==============================] - 0s 949us/step - loss: 0.1417 - acc: 0.9800\n",
      "Epoch 178/200\n",
      "150/150 [==============================] - 0s 977us/step - loss: 0.1407 - acc: 0.9800 0s - loss: 0.1434 - acc: 0.976\n",
      "Epoch 179/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1408 - acc: 0.9800\n",
      "Epoch 180/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.1402 - acc: 0.972 - 0s 1ms/step - loss: 0.1403 - acc: 0.9800\n",
      "Epoch 181/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1395 - acc: 0.9800\n",
      "Epoch 182/200\n",
      "150/150 [==============================] - 0s 944us/step - loss: 0.1390 - acc: 0.9800\n",
      "Epoch 183/200\n",
      "150/150 [==============================] - 0s 983us/step - loss: 0.1396 - acc: 0.9800\n",
      "Epoch 184/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1373 - acc: 0.9800\n",
      "Epoch 185/200\n",
      "150/150 [==============================] - 0s 984us/step - loss: 0.1376 - acc: 0.9800\n",
      "Epoch 186/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1369 - acc: 0.9800A: 0s - loss: 0.1302 - acc: 0.983\n",
      "Epoch 187/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1361 - acc: 0.9800A: 0s - loss: 0.1307 - acc: 0.983 - ETA: 0s - loss: 0.1422 - acc: 0.972\n",
      "Epoch 188/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1358 - acc: 0.9800\n",
      "Epoch 189/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1356 - acc: 0.9800\n",
      "Epoch 190/200\n",
      "150/150 [==============================] - 0s 985us/step - loss: 0.1343 - acc: 0.9800\n",
      "Epoch 191/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.1192 - acc: 0.981 - 0s 1ms/step - loss: 0.1337 - acc: 0.9800\n",
      "Epoch 192/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1337 - acc: 0.9800\n",
      "Epoch 193/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1334 - acc: 0.9800\n",
      "Epoch 194/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1331 - acc: 0.9800\n",
      "Epoch 195/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1319 - acc: 0.9800\n",
      "Epoch 196/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1319 - acc: 0.9800\n",
      "Epoch 197/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1307 - acc: 0.9800\n",
      "Epoch 198/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1307 - acc: 0.9867\n",
      "Epoch 199/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1318 - acc: 0.9800\n",
      "Epoch 200/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1293 - acc: 0.9800\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(x, y, epochs=200, batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "History对象会被模型的fit方法返回。用dir(history)的方式，可以得到History对象的所有属性，用vars(history)能够看到所有的属性值。\n",
    "\n",
    "具体可以参考：https://keras.io/zh/callbacks/#history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.5524308999379475,\n",
       " 2.217859665552775,\n",
       " 1.9382617394129436,\n",
       " 1.7239010175069174,\n",
       " 1.5819951931635539,\n",
       " 1.486048420270284,\n",
       " 1.4173336664835612,\n",
       " 1.3688759565353394,\n",
       " 1.323557710647583,\n",
       " 1.285457436243693,\n",
       " 1.2532981475194296,\n",
       " 1.2227479537328085,\n",
       " 1.1967514117558797,\n",
       " 1.1723539352416992,\n",
       " 1.1520002047220865,\n",
       " 1.1328341484069824,\n",
       " 1.116115434964498,\n",
       " 1.101676638921102,\n",
       " 1.0855707248051962,\n",
       " 1.0703706741333008,\n",
       " 1.0573806881904602,\n",
       " 1.0443492611249288,\n",
       " 1.0309179345766704,\n",
       " 1.018111534913381,\n",
       " 1.005714436372121,\n",
       " 0.9936993877092998,\n",
       " 0.9835897286732992,\n",
       " 0.9697873552640279,\n",
       " 0.9579905072848002,\n",
       " 0.9470068693161011,\n",
       " 0.9351459582646687,\n",
       " 0.9239410241444905,\n",
       " 0.9142089366912842,\n",
       " 0.9029357393582662,\n",
       " 0.89182524283727,\n",
       " 0.8813517451286316,\n",
       " 0.8719055453936259,\n",
       " 0.8624210238456727,\n",
       " 0.8514300942420959,\n",
       " 0.841620934009552,\n",
       " 0.8321502248446146,\n",
       " 0.8240049322446187,\n",
       " 0.8136581699053447,\n",
       " 0.805152400334676,\n",
       " 0.7958961606025696,\n",
       " 0.7879444162050883,\n",
       " 0.7792896191279094,\n",
       " 0.7711658835411072,\n",
       " 0.7638925711313883,\n",
       " 0.7557949185371399,\n",
       " 0.7473012288411458,\n",
       " 0.7400249481201172,\n",
       " 0.7328327695528666,\n",
       " 0.7255582094192505,\n",
       " 0.7184681177139283,\n",
       " 0.7120466669400533,\n",
       " 0.705533218383789,\n",
       " 0.6994260390599568,\n",
       " 0.6932889978090923,\n",
       " 0.6869229952494303,\n",
       " 0.6812210281689962,\n",
       " 0.6756292720635731,\n",
       " 0.6698734362920126,\n",
       " 0.6651987155278524,\n",
       " 0.6592507044474284,\n",
       " 0.6541192909081777,\n",
       " 0.6495343287785847,\n",
       " 0.6448185900847118,\n",
       " 0.640460753440857,\n",
       " 0.6361701846122741,\n",
       " 0.6321402609348297,\n",
       " 0.6274492224057515,\n",
       " 0.6235680262247721,\n",
       " 0.6197581152121227,\n",
       " 0.6157868663469951,\n",
       " 0.6143338163693746,\n",
       " 0.608618172009786,\n",
       " 0.6054133872191111,\n",
       " 0.6009598910808563,\n",
       " 0.5976622502009074,\n",
       " 0.5942548056443532,\n",
       " 0.5910789370536804,\n",
       " 0.5875347157319387,\n",
       " 0.5843660016854604,\n",
       " 0.5816761513551076,\n",
       " 0.5782064735889435,\n",
       " 0.5751431008179982,\n",
       " 0.5723207950592041,\n",
       " 0.5697240273157755,\n",
       " 0.5661402026812236,\n",
       " 0.5640341301759084,\n",
       " 0.560701294740041,\n",
       " 0.5581293106079102,\n",
       " 0.5549603680769603,\n",
       " 0.5524264991283416,\n",
       " 0.5502304871877034,\n",
       " 0.5467708786328633,\n",
       " 0.5448413610458374,\n",
       " 0.5420746624469757,\n",
       " 0.5391136308511099,\n",
       " 0.5368345300356547,\n",
       " 0.5345309476057688,\n",
       " 0.5317264179388682,\n",
       " 0.5296902398268382,\n",
       " 0.5267816960811615,\n",
       " 0.5251965939998626,\n",
       " 0.5221877475579579,\n",
       " 0.5196803867816925,\n",
       " 0.5173557301362356,\n",
       " 0.5149411380290985,\n",
       " 0.5129716217517852,\n",
       " 0.5107731183369955,\n",
       " 0.5097531795501709,\n",
       " 0.5062023083368937,\n",
       " 0.5045664211114248,\n",
       " 0.5015985051790873,\n",
       " 0.49979968667030333,\n",
       " 0.4976022899150848,\n",
       " 0.4962211827437083,\n",
       " 0.49374983509381615,\n",
       " 0.4914518137772878,\n",
       " 0.48954973022143045,\n",
       " 0.48734857042630514,\n",
       " 0.48538966178894044,\n",
       " 0.48317362467447916,\n",
       " 0.4810324728488922,\n",
       " 0.47908886075019835,\n",
       " 0.47855250438054403,\n",
       " 0.47585713664690654,\n",
       " 0.4736451248327891,\n",
       " 0.4715678572654724,\n",
       " 0.4696855485439301,\n",
       " 0.46772842009862264,\n",
       " 0.466444593667984,\n",
       " 0.46385645469029746,\n",
       " 0.46240501006444296,\n",
       " 0.4602379560470581,\n",
       " 0.45851913690567014,\n",
       " 0.4567262252171834,\n",
       " 0.4548473238945007,\n",
       " 0.4535813530286153,\n",
       " 0.45090416769186653,\n",
       " 0.4492281754811605,\n",
       " 0.44750598073005676,\n",
       " 0.44569750924905144,\n",
       " 0.44505469004313153,\n",
       " 0.4437819480895996,\n",
       " 0.44034010569254556,\n",
       " 0.4386813879013062,\n",
       " 0.4368030110994975,\n",
       " 0.43516792456309,\n",
       " 0.4333049178123474,\n",
       " 0.4314144134521484,\n",
       " 0.43093604445457456,\n",
       " 0.427865731716156,\n",
       " 0.42628084619839984,\n",
       " 0.4248205453157425,\n",
       " 0.4229179004828135,\n",
       " 0.4211631457010905,\n",
       " 0.4192521075407664,\n",
       " 0.4176590452591578,\n",
       " 0.4164066771666209,\n",
       " 0.41457987725734713,\n",
       " 0.4128027399381002,\n",
       " 0.4109214683373769,\n",
       " 0.40992406209309895,\n",
       " 0.4077167560656865,\n",
       " 0.4061404327551524,\n",
       " 0.4044195592403412,\n",
       " 0.4023512671391169,\n",
       " 0.4006972988446554,\n",
       " 0.39892800052960714,\n",
       " 0.3974647035201391,\n",
       " 0.39599781632423403,\n",
       " 0.39397475322087605,\n",
       " 0.39235864679018656,\n",
       " 0.3902157485485077,\n",
       " 0.3887821783622106,\n",
       " 0.3873137056827545,\n",
       " 0.38636297583580015,\n",
       " 0.3838875154654185,\n",
       " 0.38203344245751697,\n",
       " 0.3803322126468023,\n",
       " 0.37991961936155955,\n",
       " 0.37772443393866223,\n",
       " 0.3762625902891159,\n",
       " 0.37374604543050133,\n",
       " 0.37171969811121625,\n",
       " 0.3713418751955032,\n",
       " 0.36800645987192787,\n",
       " 0.3680317868789037,\n",
       " 0.36528300245602924,\n",
       " 0.36398465434710187,\n",
       " 0.36171703537305194,\n",
       " 0.3598902483781179,\n",
       " 0.3587204327185949,\n",
       " 0.35693380832672117,\n",
       " 0.35496418476104735,\n",
       " 0.35347410043080646,\n",
       " 0.3519529203573863]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history['loss']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.6.评估模型**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150/150 [==============================] - 0s 1ms/step\n",
      "0.98\n"
     ]
    }
   ],
   "source": [
    "result = model.evaluate(x, y)\n",
    "print(result[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.7.模型预测**\n",
    "\n",
    "predict输出概率矩阵，每一行对应预测值在三个类别上的概率；predict_classes输出类别值，可以全部打印出来看一下。\n",
    "\n",
    "**注意**：这里采用的是科学计数法。哪个数字小，就说明哪个概率最大。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.94676471e-01 5.32344682e-03 1.37119770e-07]\n",
      " [9.92624938e-01 7.37464847e-03 3.18816092e-07]\n",
      " [9.92137969e-01 7.86165334e-03 3.76266797e-07]\n",
      " [9.91234660e-01 8.76479223e-03 4.98782072e-07]\n",
      " [9.94500399e-01 5.49943792e-03 1.49158325e-07]\n",
      " [9.95819032e-01 4.18085558e-03 7.34049905e-08]\n",
      " [9.91707981e-01 8.29158351e-03 4.31940975e-07]\n",
      " [9.94068742e-01 5.93105145e-03 1.81369444e-07]\n",
      " [9.89160359e-01 1.08388551e-02 8.65405184e-07]\n",
      " [9.93282795e-01 6.71697874e-03 2.50305760e-07]\n",
      " [9.95990098e-01 4.00977861e-03 6.58896084e-08]\n",
      " [9.93174016e-01 6.82569528e-03 2.60931955e-07]\n",
      " [9.92531478e-01 7.46812671e-03 3.29390701e-07]\n",
      " [9.89453435e-01 1.05456924e-02 8.05948616e-07]\n",
      " [9.97298658e-01 2.70137028e-03 2.37442883e-08]\n",
      " [9.97193933e-01 2.80600181e-03 2.61933977e-08]\n",
      " [9.95835066e-01 4.16482333e-03 7.26794909e-08]\n",
      " [9.94362950e-01 5.63687505e-03 1.58996414e-07]\n",
      " [9.96677518e-01 3.32251959e-03 4.05301037e-08]\n",
      " [9.94955122e-01 5.04476717e-03 1.19313157e-07]\n",
      " [9.95505929e-01 4.49388195e-03 8.84715945e-08]\n",
      " [9.94454920e-01 5.54493908e-03 1.52372721e-07]\n",
      " [9.92757678e-01 7.24196015e-03 3.04168054e-07]\n",
      " [9.93171334e-01 6.82844687e-03 2.61204633e-07]\n",
      " [9.93154407e-01 6.84533501e-03 2.62881002e-07]\n",
      " [9.93108511e-01 6.89123478e-03 2.67470000e-07]\n",
      " [9.93343949e-01 6.65577315e-03 2.44442504e-07]\n",
      " [9.95030761e-01 4.96904412e-03 1.14735514e-07]\n",
      " [9.94846821e-01 5.15305996e-03 1.26052001e-07]\n",
      " [9.92115319e-01 7.88425282e-03 3.79076198e-07]\n",
      " [9.92367089e-01 7.63252890e-03 3.48507399e-07]\n",
      " [9.94970500e-01 5.02938684e-03 1.18374501e-07]\n",
      " [9.96249616e-01 3.75035405e-03 5.54279787e-08]\n",
      " [9.96901512e-01 3.09847132e-03 3.38402693e-08]\n",
      " [9.92887914e-01 7.11187208e-03 2.90217201e-07]\n",
      " [9.93627906e-01 6.37185108e-03 2.18351616e-07]\n",
      " [9.95978594e-01 4.02130513e-03 6.63801671e-08]\n",
      " [9.94430602e-01 5.56927687e-03 1.54109500e-07]\n",
      " [9.89565134e-01 1.04339933e-02 7.83979544e-07]\n",
      " [9.94468808e-01 5.53111313e-03 1.51391632e-07]\n",
      " [9.93961215e-01 6.03862246e-03 1.90006958e-07]\n",
      " [9.86641884e-01 1.33565729e-02 1.48899903e-06]\n",
      " [9.90312338e-01 9.68713127e-03 6.46557396e-07]\n",
      " [9.92811143e-01 7.18850922e-03 2.98387164e-07]\n",
      " [9.94637430e-01 5.36241243e-03 1.39731682e-07]\n",
      " [9.91628408e-01 8.37107934e-03 4.42757113e-07]\n",
      " [9.95231211e-01 4.76867938e-03 1.03148942e-07]\n",
      " [9.91562724e-01 8.43673758e-03 4.51815595e-07]\n",
      " [9.95699763e-01 4.30019246e-03 7.89456394e-08]\n",
      " [9.93849337e-01 6.15046918e-03 1.99251133e-07]\n",
      " [1.68828981e-03 9.36921954e-01 6.13897815e-02]\n",
      " [1.16874801e-03 8.88732731e-01 1.10098422e-01]\n",
      " [4.18112613e-04 8.39859307e-01 1.59722596e-01]\n",
      " [1.01510668e-03 7.91316509e-01 2.07668364e-01]\n",
      " [4.33092035e-04 7.99237311e-01 2.00329587e-01]\n",
      " [5.31170517e-04 7.64119744e-01 2.35349149e-01]\n",
      " [3.98939854e-04 7.89505243e-01 2.10095733e-01]\n",
      " [2.30233278e-02 9.41304445e-01 3.56721021e-02]\n",
      " [1.32927229e-03 9.06730115e-01 9.19406563e-02]\n",
      " [1.43305270e-03 8.11262012e-01 1.87305003e-01]\n",
      " [6.57512946e-03 9.05539155e-01 8.78857076e-02]\n",
      " [1.31322118e-03 8.60617816e-01 1.38068944e-01]\n",
      " [5.04369382e-03 9.37067866e-01 5.78884780e-02]\n",
      " [3.30102950e-04 7.45802939e-01 2.53866941e-01]\n",
      " [1.34900948e-02 9.51966941e-01 3.45429964e-02]\n",
      " [2.84973648e-03 9.44308519e-01 5.28417043e-02]\n",
      " [2.83762027e-04 6.73322916e-01 3.26393425e-01]\n",
      " [6.84887310e-03 9.47603941e-01 4.55471799e-02]\n",
      " [1.19496995e-04 5.78689694e-01 4.21190798e-01]\n",
      " [5.27065480e-03 9.27784383e-01 6.69449344e-02]\n",
      " [4.43542813e-05 4.46292132e-01 5.53663492e-01]\n",
      " [5.05576143e-03 9.42445993e-01 5.24982437e-02]\n",
      " [4.94449450e-05 4.89817202e-01 5.10133326e-01]\n",
      " [6.41449122e-04 8.24205518e-01 1.75153002e-01]\n",
      " [2.99803680e-03 9.35953856e-01 6.10481538e-02]\n",
      " [2.14849343e-03 9.28845167e-01 6.90062568e-02]\n",
      " [4.57724469e-04 8.34672511e-01 1.64869770e-01]\n",
      " [7.13416593e-05 5.95324099e-01 4.04604614e-01]\n",
      " [4.01967001e-04 7.50947833e-01 2.48650163e-01]\n",
      " [3.75455208e-02 9.47748601e-01 1.47058768e-02]\n",
      " [5.58722578e-03 9.24447715e-01 6.99649900e-02]\n",
      " [1.14406962e-02 9.47059751e-01 4.14996184e-02]\n",
      " [6.31963462e-03 9.41111624e-01 5.25687635e-02]\n",
      " [1.06459884e-05 2.84841985e-01 7.15147376e-01]\n",
      " [2.10903207e-04 6.12479925e-01 3.87309194e-01]\n",
      " [6.98613236e-04 8.23896110e-01 1.75405264e-01]\n",
      " [6.78140146e-04 8.65328610e-01 1.33993208e-01]\n",
      " [6.28690643e-04 8.10772240e-01 1.88599005e-01]\n",
      " [2.93638417e-03 9.05248463e-01 9.18152332e-02]\n",
      " [1.49171881e-03 8.36113989e-01 1.62394270e-01]\n",
      " [6.20670384e-04 7.60373235e-01 2.39006117e-01]\n",
      " [5.88049181e-04 8.11638653e-01 1.87773272e-01]\n",
      " [3.77642876e-03 9.20566201e-01 7.56574571e-02]\n",
      " [2.16148850e-02 9.42051709e-01 3.63334380e-02]\n",
      " [1.18432741e-03 8.31211746e-01 1.67603970e-01]\n",
      " [3.42780119e-03 9.20726955e-01 7.58452117e-02]\n",
      " [1.93516223e-03 8.82729828e-01 1.15335003e-01]\n",
      " [2.40619970e-03 9.19367015e-01 7.82267377e-02]\n",
      " [6.05691671e-02 9.23895061e-01 1.55358352e-02]\n",
      " [2.29848782e-03 8.90814364e-01 1.06887169e-01]\n",
      " [2.83160340e-09 1.05904108e-02 9.89409566e-01]\n",
      " [1.09718565e-06 1.07284226e-01 8.92714679e-01]\n",
      " [1.73980467e-07 7.96992853e-02 9.20300603e-01]\n",
      " [7.71004352e-07 1.17034853e-01 8.82964313e-01]\n",
      " [4.00841671e-08 3.49234156e-02 9.65076506e-01]\n",
      " [1.14150946e-08 3.13222371e-02 9.68677759e-01]\n",
      " [7.50090157e-06 1.70746550e-01 8.29245925e-01]\n",
      " [1.59927950e-07 8.77512544e-02 9.12248552e-01]\n",
      " [1.89574308e-07 7.15928599e-02 9.28406954e-01]\n",
      " [3.30021201e-08 4.20824029e-02 9.57917571e-01]\n",
      " [1.05643039e-05 3.21531564e-01 6.78457916e-01]\n",
      " [1.38912833e-06 1.42747775e-01 8.57250810e-01]\n",
      " [7.76942954e-07 1.29828528e-01 8.70170653e-01]\n",
      " [4.31631548e-07 6.73701465e-02 9.32629406e-01]\n",
      " [5.58718831e-08 2.86844708e-02 9.71315503e-01]\n",
      " [4.77567937e-07 9.25073177e-02 9.07492161e-01]\n",
      " [2.67668474e-06 2.02810615e-01 7.97186673e-01]\n",
      " [5.02110957e-08 6.65311366e-02 9.33468819e-01]\n",
      " [2.00303218e-10 5.32879960e-03 9.94671166e-01]\n",
      " [8.01685928e-06 2.44799480e-01 7.55192578e-01]\n",
      " [1.65149089e-07 7.16554970e-02 9.28344250e-01]\n",
      " [1.47259300e-06 1.11692198e-01 8.88306320e-01]\n",
      " [8.60803429e-09 2.86217649e-02 9.71378267e-01]\n",
      " [1.58930561e-05 3.38030994e-01 6.61953092e-01]\n",
      " [5.22785911e-07 1.13520935e-01 8.86478543e-01]\n",
      " [1.52833195e-06 2.11514637e-01 7.88483858e-01]\n",
      " [2.76037663e-05 3.96791875e-01 6.03180528e-01]\n",
      " [2.39893634e-05 3.80730003e-01 6.19246066e-01]\n",
      " [1.00919316e-07 4.87629883e-02 9.51236904e-01]\n",
      " [7.72012481e-06 3.71950209e-01 6.28042042e-01]\n",
      " [2.28490208e-07 9.98918042e-02 9.00108039e-01]\n",
      " [1.33089861e-06 2.59844214e-01 7.40154445e-01]\n",
      " [5.20384198e-08 3.62081043e-02 9.63791847e-01]\n",
      " [4.16675575e-05 4.84854221e-01 5.15104055e-01]\n",
      " [2.58502223e-06 1.83184609e-01 8.16812813e-01]\n",
      " [6.06492776e-08 6.05454743e-02 9.39454496e-01]\n",
      " [7.03812759e-08 4.17599641e-02 9.58239973e-01]\n",
      " [2.89356103e-06 2.05667213e-01 7.94329882e-01]\n",
      " [3.20354593e-05 4.05233353e-01 5.94734609e-01]\n",
      " [2.17245952e-06 2.00464427e-01 7.99533427e-01]\n",
      " [6.93998032e-08 4.50891741e-02 9.54910696e-01]\n",
      " [2.99786734e-06 2.14507639e-01 7.85489321e-01]\n",
      " [1.09718565e-06 1.07284226e-01 8.92714679e-01]\n",
      " [4.37655885e-08 4.04731669e-02 9.59526777e-01]\n",
      " [3.81526561e-08 3.56237143e-02 9.64376330e-01]\n",
      " [8.68914185e-07 1.24557942e-01 8.75441194e-01]\n",
      " [3.03648585e-06 1.79531932e-01 8.20464969e-01]\n",
      " [3.69291297e-06 2.16973528e-01 7.83022821e-01]\n",
      " [3.25413822e-07 7.69931227e-02 9.23006594e-01]\n",
      " [6.07401580e-06 2.27303028e-01 7.72690892e-01]]\n"
     ]
    }
   ],
   "source": [
    "proba = model.predict(x)\n",
    "print(proba[0:150])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 2 1\n",
      " 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n"
     ]
    }
   ],
   "source": [
    "classes = model.predict_classes(x)\n",
    "print(classes[0:150])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.模型的保存和导入"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6.1.模型保存**\n",
    "\n",
    "使用save可以保存训练好的模型，下次导入即可使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./model/2-model-vv.h5')   # HDF5文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6.2.模型导入**\n",
    "\n",
    "使用keras.models的load_model语句载入模型，就可以直接用这个模型来做预测了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "import pandas as pd\n",
    "model = load_model('./model/2-model-vv.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "请输入数据，用“,”分开:5.5,2.6,4.4,1.2\n"
     ]
    }
   ],
   "source": [
    "#请输入数据，如：5.5,2.6,4.4,1.2\n",
    "s=input(\"请输入数据，用“,”分开:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['5.5', '2.6', '4.4', '1.2']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i_data=s.split(',')\n",
    "i_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#生成一个空的DataFrame，将输入的列表添加为新行\n",
    "x=pd.DataFrame(columns=['Sepal.Length','Sepal.Width','Petal.Length','Petal.Width'])\n",
    "x.loc[0]=i_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#对应的结果，应该是1，即“Species_Iris-versicolor”\n",
    "model.predict_classes(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
